\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{pdflscape}
\usepackage[margin=2cm, left=3cm]{geometry}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tocbasic}
\usepackage{fancyhdr}
\usepackage{float} 
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[english, ngerman]{babel}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{color, colortbl}
\usepackage{array}
\usepackage{diagbox}
\usepackage{trfsigns}
\usepackage{pgfplots}
\usepackage{multirow}
\usepackage[
    backend=biber,		
    bibwarn=true,
    bibencoding=utf8,	% wenn .bib in utf8, sonst ascii
    sortlocale=de_DE,
    style=numeric-comp,
]{biblatex}
\usepackage{csquotes}

\addbibresource{bibliographie.bib}

\selectlanguage{ngerman}

\hypersetup{%
    colorlinks=true, 		% Aktivieren von farbigen Links im Dokument
    linkcolor=black, 	    % Farbe festlegen
    citecolor=black,
    filecolor=black,
    menucolor=black,
    urlcolor=black,
    linktoc=all,            % Seitenzahlen und Text klickbar
    bookmarksnumbered=true 	% Überschriftsnummerierung im PDF Inhalt anzeigen.
}

\newcommand\todo[1]{\textit{\textcolor{red}{\\TODO: #1}}}

\DeclareNewTOC[%
 forcenames,
 type=formel,
 name={Formel},%
 listname={Formelverzeichnis}
]{for}
\newcommand*{\formelentry}[1]{%
     \addcontentsline{for}{formel}{\protect\numberline{\theequation} #1}%
    }



\title{Master Autonomes Fahren - Mathematik Zusammenfassung}
\author{Marcel Wagner}
\date{\today}

\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{LightRed}{rgb}{1,0.6,0.6}
\newcolumntype{x}{>{\columncolor{LightCyan}}l}


\begin{document}
%\pagenumbering{Roman}
\maketitle
%\tableofcontents
%\newpage
%\listoffigures
%\listoffigures
%\listofformels
%\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Statistik}
\begin{table}[H]
	\centering
	\begin{tabular}{|l|p{0.7\textwidth}|}
		\hline
		Arithmetisches Mittel & \(\displaystyle \bar{x} := \frac{x_1 + ... + x_n}{n} = \frac{1}{n}\sum_{i=1}^{n}{x_i}\)\\\hline
		Mittlerer Abstand & \(\displaystyle \frac{1}{n}\sum_{i=1}^{n}{|x_i-\bar{x}|}\) \newline Der mittlere Abstand wird nichtsehr häufig verwendet, da das Rechnen mit Beträgen sehr mühsam ist.\newline Die Varianz (durchschnittliche quadratische Abweichung) eignet sich besser.\\\hline
		Varianz & \(\displaystyle s_x^2= \frac{1}{n-1}\sum_{i=1}^n{(x_i-\bar{x})^2}\)\\\hline
		Standartabweichung & \(\displaystyle s_x = \sqrt{s_x^2}\)\\\hline
		Kovarianz & \(\displaystyle s_{xy}:=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\) \\\hline
		Korrelationskoeffizient & \(\displaystyle r_{xy}:=\frac{s_{xy}}{s_x\cdot s_y}\)\newline\textbullet~Wenn $r_{xy}=0$, dann sind $X$ und $Y$ \textbf{unkorreliert} \newline\textbullet~Wenn $X$ und $Y$ \textbf{unabhängig} sind, so gilt $r_{xy}=0$. Dieser Satz ist nicht umkehrbar!\newline\textbullet~$r_{xy}=\pm 1$, dann sind $X$ und $Y$ \textbf{perfekt korreliert}\\\hline
		Regressionsgerade & \(\displaystyle y=ax + b\) \newline \(\displaystyle a = \frac{s_{xy}}{s_x^2}\) \newline \(\displaystyle b = \bar{y} - a\bar{x}\)\\\hline
		Bestimmtheitsmaß & \(\displaystyle R^2 = \frac{\sum_{i=1}^n (\hat{y}_i-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i-\bar{y})^2}=r^2_{xy} \) \newline mit Arithmetischem Mittel $\bar{y}$ und Ausgleichsgerade $\hat{y}_i = y(x_i) = a+bx_i$\\\hline
	\end{tabular}
\end{table}

%\subsection{Arithmetisches Mittel}
%\begin{equation*}
%	\bar{x} := \frac{x_1 + ... + x_n}{n} = \frac{1}{n}\sum_{i=1}^{n}{x_i}
%\end{equation*}
%\subsection{Mittlerer Abstand}
%Der mittlere Abstand wird nicht sehr häufig verwendet, da das Rechnen mit Beträgen sehr mühsam ist. Die Varianz (durchschnittliche quadratische Abweichung) eignet sich besser.
%\begin{equation*}
%	\frac{1}{n}\sum_{i=1}^{n}{|x_i-\bar{x}|}
%\end{equation*}
%\subsection{Varianz}
%\begin{equation*}
%	s_x^2= \frac{1}{n-1}\sum_{i=1}^n{(x_i-\bar{x})^2}
%\end{equation*}
%\subsection{Standartabweichung}
%\begin{equation*}
%	s_x = \sqrt{s_x^2}
%\end{equation*}
%\subsection{Kovarianz}
%\begin{equation*}
%	s_{xy}:=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})
%\end{equation*}
%\subsection{Korrelationskoeffizient}
%\begin{equation*}
%	r_{xy}:=\frac{s_{xy}}{s_x\cdot s_y}
%\end{equation*}
%\begin{itemize}
%	\item Wenn $r_{xy}=0$, dann sind $X$ und $Y$ \textbf{unkorreliert}
%	\item Wenn $X$ und $Y$ \textbf{unabhängig} sind, so gilt $r_{xy}=0$. Dieser Satz ist nicht umkehrbar!
%	\item Wenn $r_{xy}=\pm 1$, dann sind $X$ und $Y$ \textbf{perfekt korreliert}
%\end{itemize}
%\subsection{Regressionsgerade}
%\begin{equation*}
%	y = a + bx
%\end{equation*}
%\begin{equation*}
%	b = \frac{s_{xy}}{s_x^2}
%\end{equation*}
%\begin{equation*}
%	a = \bar{y} - b\bar{x}
%\end{equation*}
%\subsection{Bestimmtheitsmaß}
%\begin{equation*}
%	R^2 = \frac{\sum_{i=1}^n (\hat{y}_i-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i-\bar{y})^2}
%\end{equation*}
%mit Arithmetischem Mittel $\bar{y}$ und Ausgleichsgerade $\hat{y}_i = y(x_i) = a+bx_i$.
%\begin{equation*}
%	R^2 = r_{xy}^2
%\end{equation*}
\section{Wahrscheinlichkeitsrechnung}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Fakultät & \(\displaystyle n! = n \cdot (n-1) \cdot \dotsc \cdot 2 \cdot 1 \)\\\hline
		Binominalkoeffizient & \(\displaystyle {n \choose k} = \frac{n!}{k!\cdot(n-k)!}\)\\\hline
	\end{tabular}	
\end{table}
%\subsection{Fakultät}
%\begin{equation*}
%	n! = n \cdot (n-1) \cdot \dotsc \cdot 2 \cdot 1 
%\end{equation*}
%\subsection{Binomialkoeffizient}
%\begin{equation*}
%	{n \choose k} = \frac{n!}{k!\cdot(n-k)!}
%\end{equation*}
\subsection{Kugeln Ziehen}
\begin{table}[H]
	\centering
	\begin{tabular}{|c||c|c|}
		\hline
		 & mit Reihenfolge & ohne Reihenfolge\\\hline
		mit Zurücklegen & $n^k$ & ${n+k-1 \choose k}$\\\hline
		ohne Zurücklegen & $\frac{n!}{(n-k)!}$ & ${n \choose k}$\\\hline
	\end{tabular}
\end{table}

\subsection{Menge}
Unter einer Menge verstehen wir die Zusammenfassung unterscheidbarer Elemente zu einer Gesamtheit.
\begin{table}[H]
	\centering
	\begin{tabular}{|l|p{0.7\textwidth}|}
		\hline
		Gleichheit & $A = B:\Leftrightarrow$ $A$ und $B$ besitzen die gleichen Elemente.\\\hline
		Teilmenge & $A \subset B:\Leftrightarrow$ wenn alle Elemente von $A$ auch in $B$ sind, dann ist $A$ eine Teilmenge von $B$ oder auch $B$ die Obermenge von $A$.\newline Jede Menge ist Teilmenge von sich selbst.\\\hline
		Potenzmenge & Die Potenzmenge $\mathcal{P}(X)$ ist eine Menge welche aus allen Teilmengen von $U \subseteq X$ besteht.\\\hline
		Mächtigkeit & $|A|:=$ Zahl der Elemente von A.\\\hline
		Vereinigung & $A\cup B:=$ Menge aus allen Elementen welche in $A$ oder in $B$ oder in beiden enthalten sind.\\\hline
		Schnitt & $A\cap B:=$ Menge aus allen Elementen welche in $A$ und in $B$ enthalten sind.\\\hline
		Differenz & $A\setminus B:=$ Menge aus allen Elementen welche zu $A$ aber \textbf{nicht} zu $B$ gehören.\\\hline
		Komplement & $A^C:=$ Menge aus allen Elementen welche \textbf{nicht} zu $A$ gehören.\\\hline
		Kartesisches Produkt & \(\displaystyle A \times B:={(a, b):a\in A, b\in B}\)\\\hline
	\end{tabular}
\end{table}
%\subsubsection{Gleichheit}
%$A = B:\Leftrightarrow$ $A$ und $B$ besitzen die gleichen Elemente.
%\subsubsection{Teilmenge}
%$A \subset B:\Leftrightarrow$ wenn alle Elemente von $A$ auch in $B$ sind, dann ist $A$ eine Teilmenge von $B$ oder auch $B$ die Obermenge von $A$.\\
%Jede Menge ist Teilmenge von sich selbst. 
%\subsubsection{Potenzmenge}
%Die Potenzmenge $\mathcal{P}(X)$ ist eine Menge welche aus allen Teilmengen von $U \subseteq X$ besteht.
%\subsubsection{Mächtigkeit}
%$|A|:=$ Zahl der Elemente von A.
%\subsubsection{Vereinigung}
%$A\cup B:=$ Menge aus allen Elementen welche in $A$ oder in $B$ oder in beiden enthalten sind.
%\subsubsection{Schnitt}
%$A\cap B:=$ Menge aus allen Elementen welche in $A$ und in $B$ enthalten sind.
%\subsubsection{Differenz}
%$A\setminus B:=$ Menge aus allen Elementen welche zu $A$ aber \textbf{nicht} zu $B$ gehören.
%\subsubsection{Komplement}
%$A^C:=$ Menge aus allen Elementen welche \textbf{nicht} zu $A$ gehören.
%\subsubsection{Kartesisches Produkt}
%\begin{equation*}
%	A \times B:={(a, b):a\in A, b\in B}
%\end{equation*}
\subsubsection{$\sigma$-Algebra}
Eine Teilmenge einer Potenzmenge (Menge von Teilmengen, $\mathcal{A}\subseteq\mathcal{P}(\Omega)$) heißt $\sigma$-Algebra wenn sie folgende Bedingungen erfüllt:
\begin{itemize}
	\item Die Teilmenge $\mathcal{A}$ der Potenzmenge $\mathcal{P}(\Omega)$ enthält die Grundmenge $\Omega$.
	\item Das Komplement $A^\mathrm{C}$ eines Elements der Teilmenge $A \in\mathcal{A}$ ist gleich der Differenz aus Grundmenge und Element $A^\mathrm{C} := \Omega\setminus A$. Stabilität des Komplements.
	\item Sind die Mengen in der Teilmenge der Potenzmenge $A_1,A_2,A_3,... \in \mathcal{A}$ enthalten, so ist auch die Vereinigung aller Mengen in der Teilmenge der Potenzmenge enthalten $\bigcup\limits_{n\epsilon \mathbb{N}}A_n \in \mathcal{A}$
	\item Alle vorangegangenen Mengenoperationen können auf die Teilmengen angewendet werden. 
\end{itemize}

\subsection{Zufallsexperiment}
\begin{itemize}
	\item Genau festgelegte Bedingungen
	\item Zufälliger Ausgang
	\item Beliebig oft wiederholbar
	\item Ein Versuch bezeichnet einen Vorgang bei dem mehrere Ergebnisse (Elementarereignis) eintreten können
	\item Menge aller Elementarereignisse wird als Ergebnismenge (Ergebnisraum) $\Omega$ bezeichnet
\end{itemize}
\subsection{Ereignis}
\begin{itemize}
	\item Eine Teilmenge $A \subset \Omega$ heißt Ereignis
	\item $A = \emptyset$ unmögliches Ereignis
	\item $A = \Omega$ sicheres Ereignis
\end{itemize}
\subsubsection{Disjunkte Ereignisse}
Zwei ereignisse sind disjunkt (unvereinbar) wenn deren Schnitt gleich der leeren Menge ist $A \cap B = \emptyset$.
\subsubsection{Unabhängige Ereignisse}
Zwei Ereignisse heißen \textbf{unabhängig} wenn gilt: 
\begin{equation*}
	P(A\cap B) = P(A) \cdot P(B)
\end{equation*}
Sie heißen \textbf{abhängig} wenn sie nicht unabhängig sind.\\
Für unabhängige Ereignisse gilt:
\begin{equation*}
	P(A) = \frac{P(A\cap B)}{P(B)} \text{\quad bzw. \quad} P(B)=\frac{P(A\cap B)}{P(A)}
\end{equation*} 
\subsection{Axiome der Wahrscheinlichkeitsrechnung}
Die Funktion $P$ ordnet jedem Ereignis $A$ eine Wahrscheinlichkeit $P(A)$ zu.
\begin{enumerate}
	\item[(I)] Für jedes Ereignis $A\subset \Omega$ gilt $0\leq P(A) \leq 1$
	\item[(I')] Für das unmögliche Ereignis gilt $P(\emptyset) = 0$
	\item[(II)] Für das sichere Ereignis $\Omega$ gilt $P(\Omega) = 1$
	\item[(II')] Für ein Ereignis $A \subset \Omega$ gilt $P(A^C) = 1- P(A)$
	\item[(III)] Für disjunkte Ereignisse $A$ und $B$ gilt $P(A\cup B) = P(A)+P(B)$ 
	\item[(III')] Für zwei Ereignisse $A,B \subset \Omega$ gilt $P(A\cup B)= P(A)+P(B)-P(A\cap B)$ 
\end{enumerate}
\subsection{Laplace Experiment}
Endlich viele Elementarereignisse welche alle gleich wahrscheinlich sind.\\
Satz von Laplace:
\begin{equation*}
	P(A) = \frac{|A|}{|\Omega|}=\frac{\text{Anzahl der Elementarereignisse in $A$}}{\text{Anzahl aller möglichen Elementarereignisse}}
\end{equation*}
\subsection{Bedingte Wahrscheinlichkeit}
"Wahrscheinlichkeit von A gegeben B".
\begin{equation*}
	P(A|B) := \frac{P(A\cap B)}{P(B)}
\end{equation*}
Sind $A, B \subset \Omega$ \textbf{unabhängige} Ereignisse gilt:
\begin{equation*}
	P(A|B) = P(A)
\end{equation*}
Sind $A, B \subset \Omega$ \textbf{abhängige} Ereignisse gilt:
\begin{equation*}
	P(A|B) \neq P(A)
\end{equation*}
\(\displaystyle \)
\begin{table}[H]
	\centering
	\begin{tabular}{|p{0.3\textwidth}|p{0.7\textwidth}|}
		\hline
		Multiplikationssatz & \(\displaystyle P(A\cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)\) \\\hline
		Satz der totalen Wahrscheinlichkeit & Der Ergebnisraum ist gegeben durch $\Omega = \bigcup\limits_{j=1}^\infty B_j$ mit $P(B_j)>0$ und alle $j$ sind paarweise Disjunkt $B_i \cap B_j = \emptyset$ für $i\neq j$ \newline \(\displaystyle P(A) = \sum_{j=1}^\infty P(A|B_j) \cdot P(B_j)\) \newline Für den Spezialfall $\Omega = B\cup B^C$ gilt: \newline \(\displaystyle P(A) = P(B) \cdot P(A|B) + P(B^C) \cdot P(A|B^C)\) \\\hline
		Satz von Bayes & Besteht aus dem Multiplikationssatz \& der totalen Wahrscheinlichkeit:\newline\(\displaystyle P(A|B)=\frac{P(A)\cdot P(B|A)}{P(B)}\)\newline \(\displaystyle P(A|B)=\frac{P(A)\cdot P(B|A)}{P(A)\cdot P(B|A) + P(A^C)\cdot P(B|A^C)}\) \\\hline 
	\end{tabular}
\end{table}
%\subsubsection{Multiplikationssatz}
%\begin{equation*}
%	P(A\cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)
%\end{equation*}
%\subsubsection{Satz der totalen Wahrscheinlichkeit}
%Der Ergebnisraum ist gegeben durch $\Omega = \bigcup\limits_{j=1}^\infty B_j$ mit $P(B_j)>0$ und alle $j$ sind paarweise Disjunkt $B_i \cap B_j = \emptyset$ für $i\neq j$
%\begin{equation*}
%	P(A) = \sum_{j=1}^\infty P(A|B_j) \cdot P(B_j)
%\end{equation*}
%Für den Spezialfall $\Omega = B\cup B^C$ gilt:
%\begin{equation*}
%	P(A) = P(B) \cdot P(A|B) + P(B^C) \cdot P(A|B^C)
%\end{equation*}
%\subsubsection{Satz von Bayes}
%Besteht aus dem Multiplikationssatz \& der totalen Wahrscheinlichkeit:
%\begin{equation*}
%	P(A|B)=\frac{P(A)\cdot P(B|A)}{P(B)}= \frac{P(A)\cdot P(B|A)}{P(A)\cdot P(B|A) + P(A^C)\cdot P(B|A^C)}
%\end{equation*}
\subsection{Zufallsvariablen}
Eine Zufallsvariable ist eine Abbildung des Ergebnisraums auf den reellen Zahlenraum $\Omega\longmapsto\mathbb{R}$. Die Zufallsvariable ordnet jedem Elementarereignis eine reelle Zahl zu.\\
Zwei Zufallsvariablen sind \textbf{unabhängig} wenn gilt:
\begin{equation*}
	P(X\in A, Y\in B) = P(X\in A) \cdot P(Y\in B) \text{\quad für alle\quad} A,B\subset \mathbb{R} 
\end{equation*}
Die Zufallsvariablen heißen \textbf{abhängig} wenn sie nicht unabhängig sind.

\begin{table}[H]
	\centering
	\begin{tabular}{|p{0.15\textwidth}|p{0.4\textwidth}|p{0.4\textwidth}|}
		\hline
		 & Diskret & Stetig \\\hline\hline
		Zufallsvariable & Die Zufallsvariable wird \textbf{diskret} genannt wenn sie nur endlich viele oder abzählbar unendlich viele Werte annimmt. Es gilt:\newline\(\displaystyle \sum\limits_{i=1}^\infty P(X=x_i)=1\)& Eine Zufallsvariable wird \textbf{stetig} genannt, wenn es eine nicht-negative Funktion $f_X\geq 0$ mit\newline $\displaystyle \int\limits_{-\infty}^\infty f_X(x) dx = 1$\newline gibt, so dass für alle $a,b \in \mathbb{R} \cup \{-\infty, \infty\}$ mit $a\leq b$ gilt:$\displaystyle P(X\in [a,b]) = P(a\leq X \leq b) = \int\limits_a^b f_X(x)dx$\\\hline
		Wahrsch.-Fkt / Dichte & $\displaystyle p_X(x):=\begin{cases}P(X=x_i)~,x = x_i\\0 \text{, sonst}\end{cases}$\newline$\displaystyle \sum\limits_{x_i}p_X(x_i)=1=p(\Omega)$ & $f_X(x)$\\\hline
		Verteilungs-funktion & $\displaystyle F_X(x):=P(X\leq x)=\sum_{x_i\leq x}P(X=x_i)=\sum_{x_i\leq x}p_X(x_i)$& $\displaystyle F_X(x):=P(X\leq x) = \int\limits_{-\infty}^x f_X(u)du$\newline$\displaystyle P(X\in [a,b]) = P(a\leq X \leq b) = \int\limits_a^b f_X(x)dx = F_X(b) - F_X(a)$\\\hline
		\multirow{2}{10em}{Symmetrische Zufallsvariable} & \multicolumn{2}{p{0.8\textwidth}|}{Eine Zufallsvariable $X$ wird \textbf{symmetrisch} genannt, wenn es eine Symmetrieachse $c \in \mathbb{R}$ gibt, so dass für alle $d\in\mathbb{R}$ gilt:}\\ & $\displaystyle P(X=c-d) = P(X=c+d)$ & $f_X(c-d)=f_X(c+d)$ \\\hline
		Erwartungs-wert & $\displaystyle E(X):=\sum\limits_{i=1}^\infty x_i\cdot p_X(x_i)$ & $\displaystyle E(X):=\int\limits_{-\infty}^\infty x\cdot f_X(x) dx$ \\\hline
		\multirow{2}{*}{Varianz} & \multicolumn{2}{p{0.8\textwidth}|}{Eine Zufallsvariable mit Erwartungswert $\mu = E(X)$ hat die \textbf{Varianz}: $\displaystyle \sigma^2(X):=E[(X-\mu)^2]=E(X^2)-\mu^2 $} \\ &  $\displaystyle \sigma^2(X)=\left(\sum\limits_{i}x_i^2 \cdot f_X(x) \right) - \mu^2$ & $\displaystyle \sigma^2(X)=\int\limits_{-\infty}^\infty x^2 \cdot f_X(x) dx - \mu^2$ \\\hline
		Standart-abweichung & \multicolumn{2}{c|}{$\displaystyle\sigma(X) = \sqrt{\sigma^2(X)}$} \\\hline
	\end{tabular}
\end{table}

%\subsubsection{Diskrete Zufallsvariable}
%Die Zufallsvariable wird \textbf{diskret} genannt wenn sie nur endlich viele oder abzählbar unendlich viele Werte annimmt. Es gilt:
%\begin{equation*}
%	\sum\limits_{i=1}^\infty P(X=x_i)=1 
%\end{equation*}
%\subsubsection{Wahrscheinlichkeitsfunktion}
%Für die diskrete Zufallsvariable $X$ und ihre Ausprägungen lautet die Wahrscheinlichkeitsfunktion:
%\begin{equation*}
%	p_X(x):=
%	\begin{cases}
%		P(X=x_i) \text{, für } x = x_i \text{ mit Zählindex } i \in \mathbb{N}\\
%		0 \text{, sonst}
%	\end{cases}
%\end{equation*}
%\begin{equation*}
%	\sum\limits_{x_i}p_X(x_i)=1=p(\Omega)
%\end{equation*}
%\subsubsection{Verteilungsfunktion diskreter Zufallsvariablen}
%Für die diskrete Zufallsvariable $X$ und ihre Ausprägungen lautet die Verteilungsfunktion:
%\begin{equation*}
%	F_X(x):=P(X\leq x)=\sum_{x_i\leq x}P(X=x_i)=\sum_{x_i\leq x}p_X(x_i)
%\end{equation*}
%\subsubsection{Stetige Zufallsvariable}
%Eine Zufallsvariable wird \textbf{stetig} genannt, wenn es eine nicht-negative Funktion $f_X\geq 0$ mit 
%\begin{equation*}
%	\int\limits_{-\infty}^\infty f_X(x) dx = 1
%\end{equation*}
%gibt, so dass für alle $a,b \in \mathbb{R} \cup \{-\infty, \infty\}$ mit $a\leq b$ gilt:
%\begin{equation*}
%	P(X\in [a,b]) = P(a\leq X \leq b) = \int\limits_a^b f_X(x)dx
%\end{equation*}
%$f_X$ wird als \textbf{Dichtefunktion} (Wahrscheinlichkeitsdichte) der Zufallsvariable $X$ bezeichnet.
%Ihre Verteilungsfunktion $F_X$ lautet:
%\begin{equation*}
%	F_X(x):=P(X\leq x) = \int\limits_{-\infty}^x f_X(u)du
%\end{equation*}
%Außerdem gilt:
%\begin{equation*}
%	f_X = F'_X
%\end{equation*}
%Daraus folgt:
%\begin{equation*}
%	P(X\in [a,b]) = P(a\leq X \leq b) = \int\limits_a^b f_X(x)dx = F_X(b) - F_X(a)
%\end{equation*}
%\subsubsection{Symmetrische Zufallsvariable}
%Eine Zufallsvariable $X$ wird \textbf{symmetrisch} genannt, wenn es eine Symmetrieachse $c \in \mathbb{R}$ gibt, so dass für alle $d\in\mathbb{R}$ gilt:
%\begin{itemize}
%	\item für diskrete Zufallsvariablen 
%	\begin{equation*}
%		P(X=c-d) = P(X=c+d)
%	\end{equation*}
%	\item für stetige Zufallsvariablen
%	\begin{equation*}
%		f_X(c-d)=f_X(c+d)
%	\end{equation*}
%\end{itemize}
\subsubsection{Mehrdimensionale Verteilungsfunktion}
Die \textbf{Verteilungsfunktion} einer zweidimensionalen Zufallsveriablen $Z = (X_1,...,X_n)$ wird definiert durch:
\begin{equation*}
	F_Z(x_1,...,y)=P(X_1\leq x_1,..., X_n\leq x_n).
\end{equation*}
\subsubsection{Rand-Verteilungsfunktion}
Als Rand-Verteilungsfunktion einer mehrdimensionalen Zufallsvariablen $Z=(X_1,...,X_n)$ wird diejenige Funktion bezeichnet welche lediglich eine dimension betrachtet.
\begin{equation*}
	F_{X_i}(x_i)=F_Z(\infty,...,\infty,x_i,\infty,...,\infty)
\end{equation*}
Für die zweidimensionale Rand-Verteilungsfunktion ($Z=(X,Y)$, $F_Z(x,y)$) gilt:
\begin{equation*}
	F_X(x)=F_Z(x,\infty) \text{ sowie } F_Y(y) = F_Z(\infty,y)
\end{equation*}
\subsubsection{Totale Wahrscheinlichkeit}
\begin{equation*}
	f_X(x) = \int f_{X,Y}(x,y)dy = \int f_Y(y) \cdot f_X(x|Y=y) dy
\end{equation*}
Mit dieser Formel lässt sich eine Rand-Dichte aus einer gemeinsamen Dichte bestimmen, dies wird als \textbf{Marginalisierung} bezeichnet.
%\subsubsection{Erwartungswert einer Zufallsvariable}
%Für eine diskrete Zufallsvariable mit $(x_i)_{i\in \mathbb{N}}$ Ausprägungen und Wahrscheinlichkeitsfunktion $p_X$ lautet der \textbf{Erwartungswert}:
%\begin{equation*}
%	E(X):=\sum\limits_{i=1}^\infty x_i\cdot p_X(x_i)
%\end{equation*}
%Für eine stetige Zufallsvariable mit Dichte $f_X$ lautet der Erwartungswert:
%\begin{equation*}
%	E(X):=\int\limits_{-\infty}^\infty x\cdot f_X(x) dx
%\end{equation*}
\subsubsection{Transformationen von Zufallsvariablen}
\begin{itemize}
	\item Linearität:
	\begin{equation*}
		E(a\cdot X + b\cdot Y)=a\cdot E(X)+b\cdot E(Y)
	\end{equation*}
	\item Multiplikation:
	\begin{equation*}
		E(X\cdot Y) = E(X)\cdot E(Y)
	\end{equation*}
\end{itemize}
%\subsubsection{Varianz einer Zufallsvariable}
%Eine Zufallsvariable mit Erwartungswert $\mu = E(X)$ hat die \textbf{Varianz}:
%\begin{gather*}
%	\sigma^2(X):=E[(X-\mu)^2]=E(X^2)-\mu^2 \\
%	\sigma^2(X)=\int\limits_{-\infty}^\infty x^2 \cdot f_X(x) dx - \mu^2\\
%	\sigma^2(X)=\left(\sum\limits_{i}x_i^2 \cdot f_X(x) \right) - \mu^2\\
%\end{gather*}
%Die Standartabweichung lautet:
%\begin{equation*}
%	\sigma(X) = \sqrt{\sigma^2(X)}
%\end{equation*}
\subsubsection{Grenzwertsatz von Zufallsvariablen}
Für $X_1,...,X_n$ unabhängige und identisch verteilte Zufallsvariablen mit $E(X_i)=\mu$, $\sigma(X_i)=\sigma$ und $\bar{X}:=\frac{1}{n}(X_1+...+X_n)$ gilt:
\begin{gather*}
	E(\bar{X})=\mu \\
	\sigma^2(\bar{X})=\frac{\sigma^2}{n}\\
	\sigma(X)=\frac{\sigma}{\sqrt{n}}
\end{gather*}
\subsection{Quantil}
Bezeichnet das kleinste $x$ mit $F_X(x)\geq p$. \\
Spezielle Quantile sind:
\begin{itemize}
	\item $x_{0.5}$ Median
	\item $x_{0.25}, x_{0.5}, x_{0.75}$ erstes, zweites und drittes Quantil
	\item $x_{0.01}, x_{0.02}, x_{0.03}, ...$ erstes, zweites, drittes, ... Perzentil
\end{itemize}
\subsection{Diskrete Verteilungen}
\subsubsection{Bernoulli-Verteilung}
Eine Zufallsvariable wird \textbf{Bernoulli-verteilt} genannt, wenn sie nur zwei mögliche Ausprägungen (z.B. 0 \& 1) hat.  Ihre Wahrscheinlichkeit lautet:
\begin{equation*}
	p:=P(X=1) \text{ und } q:=1-p=P(X=0)
\end{equation*}
Außerdem gilt:
\begin{gather*}
	E(X)=p\\
	\sigma^2(X)=p\cdot q=p\cdot (1-p)\\
	\sigma(X)=\sqrt{p\cdot q}=\sqrt{p\cdot (1-p)}
\end{gather*}
\subsubsection{Binomialverteilung}
Eine \textbf{Binomialverteilung} $X$ bezeichnet die Anzahl der Erfolge bei $n$ identischen unabhängigen Bernoulli-Experimenten $X \sim B(n;p)$.  
\begin{gather*}
	B(n;p)(k) := p_X(k) = P(X=k):= {n \choose k}p^k\cdot (1-p)^{n-k} \text{ für } k=0,1,...,n \\
	 B(n;p)(k) := \frac{n!}{k!\cdot(n-k)!}p^k\cdot (1-p)^{n-k}
\end{gather*}
Weiter gilt:
\begin{gather*}
	E(X)=n\cdot p\\
	\sigma^2(X)=n\cdot p \cdot (1-p)\\
	\sigma(X)=\sqrt{n\cdot p\cdot (1-p)}
\end{gather*}
Eine Binmialverteilung ist für $p=0,0.5,1$ symmetrisch. Für alle anderen Werte ist sie nicht symmetrisch.\\
Aufgrund der Symmetrie gilt zudem:
\begin{equation*}
	B(n;p)(k) = B(n;1-p)(n-k) \text{ mit } n\in \mathbb{N}, p\in [0,1]
\end{equation*}
Für zwei Binominalverteilungen $X\sim B(n_1,p)$ und $Y\sim B(n_2,p)$ gilt, dass deren Summe wieder Binomialverteilt ist:
\begin{equation*}
	X+Y\sim B(n_1+n_2,p)
\end{equation*}
Eine Binomialverteilung $X$ mit seltenen Ereignissen $(p\approx0, N\gg 0)$ wird \textbf{Poissonverteilung} genannt $X\sim Po(\lambda)$. Sie wird approximiert durch:
\begin{equation*}
	p_X(k)=P(X=k):=\frac{\lambda^k}{k!}\cdot e^{-\lambda} \text{ für } k=0,1,2,3,... \text{ und mit } \lambda:=E(X)
\end{equation*}
\subsection{Stetige Verteilungen}
\subsubsection{Gleichverteilung}
Die \textbf{Gleichverteilung} $X$ auf $[a,b]\subset \mathbb{R}$ ($X\sim U([a,b])$) besitzt folgende Dichte:
\begin{equation*}
	f_X(x)=\begin{cases}
		\frac{1}{b-a} \text{, für} x\in [a,b] = \int_a^b\frac{1}{b-a}dx=\frac{b}{b-a}-\frac{a}{b-a}=1\\
		0\text{, sonst}
	\end{cases}
\end{equation*}
Es gilt:
\begin{gather*}
	E(X)=\frac{a+b}{2}\\
	\sigma^2 =\frac{(b-a)^2}{12}\\
	\sigma = \frac{b-a}{\sqrt{12}}
\end{gather*}
Die Dichte der Addition von 2 Gleichverteilungen $X+Y$ entspricht der Faltung ihrer einzelnen Dichten $f_{X+Y}(x)=f_X(x)\star f_Y(x)$.
\subsubsection{Inversionsmethode}
Es sei $X$ eine Zufallsvariable und $F_X$ ihre Verteilungsfunktion.\\
Die Funktion $F_X^{-1}$ ist die inverse Verteilungsfunktion (\textbf{Quantil-Funktion}):
\begin{equation*}
	F_X^{-1}(u):= \inf\{x\in\mathbb{R}|F(x)\geq u\}
\end{equation*}
Bedeutet, die inverse Verteilungsfunktion liefert das kleinste $x$ welches in der Verteilungsfunktion den Funktionswert $u$ überschreitet.\\
Für eine gleichverteilte Zufallsvariable (das bedeutet alle Zahlen von 0 bis 1 kommen gleich häufig vor) $U\sim U([0,1])$ gilt:
\begin{equation*}
	X:=F_X^{-1}(U) \text{ hat die Verteilungsfunktion } F_X
\end{equation*}
Erklärung: U ist von 0 bis 1 gleichverteilt (alle Zahlen (x-Achse) kommen gleich häufig vor). Nun wird jeder Wert der Zufallsvariable U in die inverse Verteilungsfunktion eingesetzt. Dadurch wird jetzt die Funktion $F_X$ nachgebildet, da immer das kleinste x der Verteilungsfunktion für den Wert von U zurückgegeben wird.
\subsection{Normalverteilung}
Die \textbf{Normalverteilung} $X$ ($X\sim N(\mu , \sigma^2)$) mit den Parametern $\mu\in\mathbb{R}$ und $\sigma\in\mathbb{R}^+$ besitzt folgende Dichte und wird auch \textbf{Gaußsche Glockenkurve} genannt:
\begin{equation*}
	f_X(x)=\frac{1}{\sigma\sqrt{2\pi}}\cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end{equation*}
Die Gaußsche Glockenkurve besitzt an der Stelle $\mu$ ein Maximum, sowie zwei Wendepunkte an den Stellen $\mu\pm\sigma$. Zudem Gilt:
\begin{gather*}
	E(X)=\mu \\
	\sigma(X)=\sigma
\end{gather*}
Falls $\mu = 0$ und $\sigma = 1$ nennt man die normalverteilte Zufallsgröße $X$ auch Standartnormalfunktion $\Phi$. Für diese gilt:
\begin{gather*}
	\Phi(-z) = 1-\Phi(z) \text{ mit } z\in\mathbb{R}
\end{gather*} 
Falls $Y:=a\cdot X+b$ mit $a,b\in\mathbb{R}$, $a\neq 0$ und $X\sim N(\mu,\sigma^2)$ dann gilt:
\begin{gather*}
	Y\sim N(a\mu+b,a^2\sigma^2)\\
	\frac{X-\mu}{\sigma}\sim N(0,1)
\end{gather*}
Daher gilt weiter:
\begin{gather*}
	P(X\leq x)=\Phi\left(\frac{x-\mu}{\sigma}\right) \text{ mit } x\in\mathbb{R}\\
	P(X\in[a,b])=P(a\leq X\leq b)=\Phi\left(\frac{b-\mu}{\sigma}\right)-\Phi\left(\frac{a-\mu}{\sigma}\right) \text{ mit } a,b\in\mathbb{R} \text{ und } a<b
\end{gather*}
Für eine Normalverteilung $X\sim N(\mu,\sigma^2)$ mit $\alpha\in\mathbb{R}^+$ gilt:
\begin{equation*}
	P(\mu-\alpha<X<\mu+\alpha)=2\Phi\left(\frac{\alpha}{\sigma}\right)-1
\end{equation*}
Wenn nun $p\in[0,1]$ liegt und $\bar{x}$ mit $\Phi(\bar{x})=\frac{p+1}{2}$ ist, so gilt mit $\alpha=\sigma\cdot\bar{x}$:
\begin{equation*}
	P(\mu-\alpha<X\mu+\alpha)=p
\end{equation*}
Daher gilt, dass
\begin{itemize}
	\item $P(\mu-\sigma\leq X\leq\mu+\sigma)\approx\frac{2}{3}$
	\item $P(\mu-2\sigma\leq X\leq\mu+2\sigma)\approx 0.95$
	\item $P(\mu-3\sigma\leq X\leq\mu+3\sigma)\approx 0.9975$
\end{itemize}
\subsection{Zentraler Grenzwertsatz}
Für $X_1,X_2,X_3,...,X_n$ unabhängig und identisch verteile Zufallsvariablen mit $E(X_i)=\mu$ und $\sigma(X_i)=\sigma$ für $i=1,...,n$ und $\bar{X}:=\frac{1}{n}\sum_{i=1}^nX_i$ gilt:
\begin{gather*}
	Z_n:=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\rightarrow N(0,1)\\
	\lim_{n\rightarrow\infty}P(Z_n\leq x)=\Phi(x)
\end{gather*}
Für Daten unabhängiger, identischer Zufallsexperimente ist deren Mittelwert annäherned normalverteilt. Deren Erwartungswert und Standartabweichung lässt sich einfach empirisch bestimmen. 
\subsection{Hypothesentests}
\begin{itemize}
	\item \textbf{Nullhypothese} $H_0$: Annahme über ein erwartetes $\mu$
	\item \textbf{Alternativhypothese} $H_1$: Gegenereignis zu $H_0$ 
\end{itemize}
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|}
			\hline
			Nullhypothese $H_0$ & Alternativhypothese $H_1$ & Art\\\hline
			$\theta = \theta_0$ & $\theta \neq \theta_0$ & zweiseitig\\\hline
			$\theta \leq \theta_0$ & $\theta > \theta_0$ & einseitig\\\hline
			$\theta \geq \theta_0$ & $\theta < \theta_0$ & einseitig\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|}
			\hline
			\diagbox{Entscheidung}{Ground Truth}&$H_0$ ist wahr & $H_1$ ist wahr\\\hline
			Annahme von $H_0$ & richtig entschieden & Fehler zweiter Art \\\hline
			Ablehnung von $H_0$  & Fehler erster Art & richtig entschieden\\\hline
		\end{tabular}
	\end{table}
\begin{itemize}
	\item \textbf{Ablehung von} $H_0$: Abweichung zwischen Prüfgröße und Annahme ist signifikant. Somit wird die Nullhypothese $H_0$ verworfen, bzw. die Alternativhypothese $H_1$ - mit einer Irrtumswahrscheilichkeit von $\alpha$ angenommen.
	\item \textbf{Annahme von } $H_0$: Es spricht nichts gegen eine Ablehnung/Verwerfung von $H_0$ damit wird diese Angenommen. 
\end{itemize}

Vorgehen zum aufstellen eines Hypothesentests:
\begin{enumerate}
	\item Aufstellen der Nullhypothese $H_0$ und Alternativhypothese $H_1$
	\item Festlegen des Signifikanzniveaus $\alpha$ (Wahrscheinlichkeit für einen Fehler erster Art)
	\item Berechnung des Streubereichs zum Signifikanzniveau $\alpha$
	\item Erhebung einer Stichprobe und berechnen der zugehörigen Prüfgröße
	\item Entscheidung über die Ablehnung oder Annahme von $H_0$
\end{enumerate}
\subsubsection{Hypothesentests bei Normalverteilungen}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{$H_0$} & \textbf{$H_1$} & \textbf{Streubreich}\\\hline
		$\mu=\mu_0$ & $\mu\neq\mu_0$ & $\left[\mu_0-z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}, \mu_0+z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}\right]$\\\hline
		$\mu\geq\mu_0$ & $\mu<\mu_0$ & $\left[\mu_0-z_{1-\alpha}\cdot\frac{\sigma}{\sqrt{n}}, \infty\right)$\\\hline
		$\mu\leq\mu_0$ & $\mu>\mu_0$ & $\left(-\infty, \mu_0+z_{1-\alpha}\cdot \frac{\sigma}{\sqrt{n}}\right]$\\\hline
	\end{tabular}
\end{table}
$z_{1-\alpha/2}$ ist hierbei derjenige Wert bei welchem $\Phi (z) = 1-\alpha/2$ wird.\\ 
Für eine Stichprobe $x_1,...,x_n$ von $X$ mit $\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$ gilt:
\begin{gather*}
	\bar{x}\in \text{ Streubereich }\Rightarrow H_0 \text{ wird akzeptiert}\\
	\bar{x}\notin \text{ Streubereich }\Rightarrow H_0 \text{ wird abgelehnt}
\end{gather*}
\section{Fourierreihen}
\subsection{Gleichanteil / Mittelwert}
\begin{equation*}
	m=\frac{\text{Integral über eine Periode}}{\text{Periode}}=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)dt
\end{equation*}
\subsection{Trigonometrisches Polynom}
Darstellung einer Funktion durch ein Vielfaches aus Sinus und Kosinus mit unterschiedlichen vielfachen an Kreisfrequenzen.
\begin{equation*}
	p_n(t)=\frac{a_0}{2}+\sum\limits_{k=1}^n(a_kcos(k\omega t)+b_ksin(k\omega t)), \quad \omega=\frac{2\pi}{T}
\end{equation*}
Man nennt diese Funktion \textbf{trigonometrisches Polynom} vom Grad $n$. $a_0,a_1,...,a_n$ und $b_1,...,b_n$ sind beliebige Zahlen, mit $a_n, b_n \neq 0$.\\
Für jede Funktion $f(t)$ mit Periode $T>0$ lässt sich als Fourierreihe darstellen.\\
Die Fourierreihe konvergiert
\begin{enumerate}
	\item gleichmäßig (gleicher Fehler an allen Stellen) gegen $f$ wenn $f$ stetig (ohne Sprungstellen) und abschnittsweise stetig differenzierbar ist,
	\item punktweise gegen $\frac{1}{2}(f(t+0)+f(t-0)$, das arithmetische Mittel aus links- und rechtsseiteigem Grenzwert, wenn $f$ aus endlich vielen stetigen Abschnitten besteht (mit Sprungstellen),
	\item und somit konvergiert sie in den abgeschlossenen Intervallen in denen $f$ stetig ist - dort sogar gleichmäßig.
\end{enumerate}
\subsection{Berechnung der Fourierkoeffizienten}
\begin{gather*}
	a_k=\frac{2}{T}\int\limits_{-\frac{T}{2}}^{\frac{T}{2}}f(t)cos(k\omega t)dt, \quad k=1,2,...\\
	b_k=\frac{2}{T}\int\limits_{-\frac{T}{2}}^{\frac{T}{2}}f(t)sin(k\omega t)dt, \quad k=1,2,...\\
\end{gather*}
Für periodische \textbf{stetige Funktionen Konvergiert} die Fourierreihe schnell, da ihre Koeffizienten $a_k,b_k$ zu $1/k^2$ proportional abklingen. Für Funktionen mit Sprungstellen klingen sie nur zu $1/k$ proportional ab.\\
Für \textbf{gerade Funktionen} (achensymmetrisch zur y-Achse) sind die Koeffizienten $b_k = 0$. Für \textbf{ungerade Funktionen} (punktsymmetrisch zum Ursprung) sind die Koeffizeienten $a_k = 0$.
\subsection{Gibbs'sches Phänomen}
Bei z.B. nachbildung einer Rechteckfunktion mittels der Fourierreihe bleibt ein Überschwinger kurz nach der Flanke von $\sim 9\%$ bestehen (und auch nicht entfernen).
\subsection{Komplexe Fourierreihe}
Darstellung einer Funktion $f$ mit Periode $T$ als unendliche Summe heißt \textbf{komplexe Fourierreihe}
\begin{equation*}
	f(t) = \sum\limits_{k=-\infty}^{\infty}c_ke^{ik\omega t}, \quad \omega=\frac{2\pi}{T}
\end{equation*}
Der zusammenhang zwischen der \textbf{komplexen Fourrierkoeffizienten} zu den reellen Fourierkoeffizienten besteht aus:
\begin{gather*}
	a_0=2\Re(c_0), \quad c_0=\frac{a_0}{2}\\
	a_k=2\Re(c_k), \quad c_k=\frac{a_k-ib_k}{2}, \quad k=1,2,3,...\\
	b_k=-2\Im(c_k), \quad c_k=\frac{a_k+ib_k}{2}, \quad k=-1,-2,-3,...
\end{gather*}
\subsection{Berechnung komplexer Fourierkoeffizienten}
\begin{equation*}
	c_k=\frac{1}{T}\int\limits_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-ik\omega t}dt, \quad k=0,\pm1,\pm2,...
\end{equation*}
\subsection{Ähnlichkeit, Zeitumkehr und Zeitverschiebung}
\textbf{Ähnlichkeit}:\\
Die Funktionen $f(t)$ und $\tilde{f}(t)=f(at)$ haben dieselben Fourierkoeffizienten. Die beiden Fourierreihen haben lediglich unterschiedliche Perioden und Kreisfrequenzen.\\
\begin{center}
\begin{tikzpicture}
	\begin{axis}[
		width=8cm,
		height=4cm,
		x axis line style={-latex},
		y axis line style={-latex},
		ymax = 1.5,xmax=5.5,
		axis lines*=center,
		ytick={0.5,1},
		xtick={1,2,3,4,5},
		xlabel near ticks,
		ylabel near ticks,
		xlabel style={at={(rel axis cs:1,0)}},
      	xlabel=$x$,
      	ylabel style={at={(rel axis cs:0,1)},rotate=-90},
      	ylabel=$y$]
		\addplot+[thick,mark=none,const plot] coordinates {(0,0) (0,1) (1,0) (2,1) (3,0) (4,1) (5,0)};
	\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
	\begin{axis}[
		width=8cm,
		height=4cm,
		x axis line style={-latex},
		y axis line style={-latex},
		ymax = 1.5,xmax=5.5,
		axis lines*=center,
		ytick={0.5,1},
		xtick={1,2,3,4,5,6},
		xlabel near ticks,
		ylabel near ticks,
		xlabel style={at={(rel axis cs:1,0)}},
      	xlabel=$x$,
      	ylabel style={at={(rel axis cs:0,1)},rotate=-90},
      	ylabel=$y$]
	\addplot+[thick,mark=none,const plot, color=red] coordinates{(0,0) (0,1) (2,0) (4,1) (6,0)};
	\end{axis}
\end{tikzpicture}
\end{center}
\textbf{Zeitumkehr}:\\
Die Funktionen $f(t)$ und $\tilde{f}(t)=f(-t)$ haben dieselbe Periode $T$ und Kreisfrequenz $\omega$. Zwischen den Fourierkoeffizienten, Spektrum und Phase besteht der Zusammenhang:
\begin{equation*}
	\tilde{a_k}=a_k,\tilde{b_k}=-b_k,\tilde{c_k}=\overline{c_k},\tilde{A_k}=A_k,\tilde{\varphi_k}=\varphi_k, 
\end{equation*}
\begin{center}
\begin{tikzpicture}
	\begin{axis}[
		width=8cm,
		height=4cm,
		x axis line style={-latex},
		y axis line style={-latex},
		ymax = 1.5,xmax=4.5,
		axis lines*=center,
		ytick={0.5,1},
		xtick={-2, -1, 1,2,3,4},
		xlabel near ticks,
		ylabel near ticks,
		xlabel style={at={(rel axis cs:1,0)}},
      	xlabel=$x$,
      	ylabel style={at={(rel axis cs:0,1)},rotate=-90},
      	ylabel=$y$]
		\addplot [thick, color=blue] coordinates {(-2,1) (-1,0) (0,0) (0,1) (1,0) (2,0) (2,1) (3,0) (4,0)};
	\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
	\begin{axis}[
		width=8cm,
		height=4cm,
		x axis line style={-latex},
		y axis line style={-latex},
		ymax = 1.5,xmax=4.5,
		axis lines*=center,
		ytick={0.5,1},
		xtick={-2, -1, 1,2,3,4},
		xlabel near ticks,
		ylabel near ticks,
		xlabel style={at={(rel axis cs:1,0)}},
      	xlabel=$x$,
      	ylabel style={at={(rel axis cs:0,1)},rotate=-90},
      	ylabel=$y$]
		\addplot [thick, color=red] coordinates {(-2,0) (-1,0) (0,1) (0,0) (1,0) (2,1) (2,0) (3,0) (4,1)};
	\end{axis}
\end{tikzpicture}
\end{center}
\textbf{Zeitverschiebung}:\\
Die Funktionen $f(t)$ und $\tilde{f}(t)=f(t-t_0)$ haben dieselben Anplituden $\tilde{A_k}=A_k$ und Phasenwinkel $\tilde{\varphi_k}=-k\omega t_0 + \varphi_k$.\\
\begin{center}
\begin{tikzpicture}
	\begin{axis}[
		width=8cm,
		height=4cm,
		x axis line style={-latex},
		y axis line style={-latex},
		ymax = 1.5,xmax=5.5,
		axis lines*=center,
		ytick={0.5,1},
		xtick={1,2,3,4,5},
		xlabel near ticks,
		ylabel near ticks,
		xlabel style={at={(rel axis cs:1,0)}},
      	xlabel=$x$,
      	ylabel style={at={(rel axis cs:0,1)},rotate=-90},
      	ylabel=$y$]
		\addplot+[thick,mark=none,const plot, color=blue] coordinates {(0,0) (0,1) (1,0) (2,1) (3,0) (4,1) (5,0)};
		\addplot+[thick,mark=none,const plot, color=red] coordinates {(0,0) (0.5,0) (0.5,1) (1.5,0) (2.5,1) (3.5,0) (4.5,1) (5,1)};
	\end{axis}
\end{tikzpicture}
\end{center}
\section{Verallgemeinerte Funktionen}
\subsection{Heavisiede-Funktion}
\begin{equation*}
	\sigma(t)=\begin{cases}
		0 \text{ für } t<0\\
		1 \text{ für } t\geq 1
	\end{cases}
\end{equation*}
\subsection{Rechteckpuls}
\begin{equation*}
	r(t)=\sigma(t-t_0)-\sigma(t-t_1)
\end{equation*}
Der Rechteckpuls lässt die Funktion $f$ außerhalb des Intervalls $[t_0,t_1]$ ausblenden, was zu einer neuen Funktion $g(t)$ führt:
\begin{equation*}
	g(t):=f(t)r(t)=\begin{cases}
		0 \text{ für } t<t_0\\
		f(t) \text{ für } t_0\leq t \leq t_1\\
		0 \text{ für } t>t_1
	\end{cases}
\end{equation*}
\subsection{Dirac-Puls}
Die Rechteckfunktion $d_\varepsilon$ mit konstantem Flächeninhalt 1:
\begin{equation*}
	d_\varepsilon=\frac{1}{\varepsilon}(\sigma(t)-\sigma(t-\varepsilon))
\end{equation*}
Wird im Grenzwert zum \textbf{Dirac-Puls}/\textbf{Dirac-Distribution}:
\begin{equation*}
	\delta(t)=\lim_{\varepsilon\rightarrow 0}d_\varepsilon(t)
\end{equation*}
Wenn man eine Funktion $f$ mit dem Dirac-Puls $\delta(t-t0)$ multipliziert, so werden alle Funktionswerte außerhalb $t_0$ ausgeblendet.
\begin{gather*}
	\int_{-\infty}^{\infty}f(t)\delta(t-t_0)dt=f(t_0)\\
	\int_{-\infty}^{\infty}\delta(t-t_0)dt=1
\end{gather*}
Die \textbf{varallgemeintere Ableitung} der Heaviside-Funktion ist der Dirac-Puls:
\begin{gather*}
	\dot{\sigma}(t)=\delta(t)\\
	\int\delta(t)dt=\sigma(t)+C
\end{gather*}
\subsection{Faltung}
\begin{equation*}
	h(t) = f(t)\star g(t)=\int_{-\infty}^\infty f(\tau)g(t-\tau)d\tau
\end{equation*}
Rechenregeln (mit Funktionen $f,g,h$ und Konstante $C$):
\begin{itemize}
	\item $f\star g = g\star f$
	\item $C(f\star g) = (Cf)\star g=f\star (Cg)$
	\item $f\star (g\star h)=(f\star g)\star h$
	\item $f\star (g + h)=(f\star g) + (f\star h)$
\end{itemize}
Eine Faltung einer Funktion $f$ mit dem Dirac-Puls $\delta$ lässt die Funktion $f$ unverändert.\\
\subsection{Einseitige Faltung}
Sind beide Funktionen $f$ und $g$ für negative Argumente null, dann berechnet sich die einseitige Faltung durch:
\begin{equation*}
	f(t)\star g(t)=\int_0^tf(\tau)g(t-\tau)d\tau
\end{equation*}
\section{Fouriertransformation}
Durch die Fouriertransformation wird einer Funktion $s$ im Zeitbereich eine Funktion $S$ im Frequenzbereich zugeordnet.
\begin{equation*}
	s(t)\laplace S(f)=\int_{-\infty}^\infty s(t)e^{-2\pi ift}dt
\end{equation*}
Eigenschaften:
\begin{itemize}
	\item \textbf{Linearität}:\\
			Eine Addition von Funktionen im Zeitbereich entspricht einer Addition der Fouriertransformierten im Frequenzbereich.\\
			Eine Multiplikation einer Funktion mit einem konstanten Faktor im Zeitbereich entspricht der Multiplikation mit demselben Faktor im Frequenzbereich.
			\begin{eqnarray*}
				s_1(t),s_2(t) & \laplace & S_1(f),S_2(f)\\
				\downarrow & & \downarrow \\
				C_1s_1(t)+C_2s_2(t) & \laplace & C_1S_1(f)+C_2S_2(f)
			\end{eqnarray*}
	\item \textbf{Zeitverschiebung}:\\
			Eine Verschiebung der Funktion $s$ um $t_0$ im Zeitbereich entspricht der Multiplikation mit $e^{-i2\pi ft_0}$ im Frequenzbereich.
			\begin{eqnarray*}
				s(t) & \laplace & S(f)\\
				\downarrow & & \downarrow \\
				s(t-t_0) & \laplace & e^{-i2\pi ft_0}S(f)
			\end{eqnarray*}
	\item \textbf{Frequenzverschiebung}:\\
			Eine Verschiebung der Funktion $S$ um $f_0$ im Frequenzbereich entspricht der Multiplikation mit $e^{-2\pi f_0 t}$ im Zeitbereich.
			\begin{eqnarray*}
				S(f) & \Laplace & s(t)\\
				\downarrow & & \downarrow \\
				S(f-f_0) & \Laplace & e^{-i2\pi f_0t}s(t)
			\end{eqnarray*}
\end{itemize}
\section{Laplacetransformation}
Durch die Laplacetransformation wird einer Funktion $f$ im Zeitbereich eine Funktion $F$ im komplexen Bildbereich zugeordnet.
\begin{equation*}
	f(t)\laplace F(s)=\int_0^\infty f(t)e^{-st}dt, \quad t\in\mathbb{R}, s\in\mathbb{C}
\end{equation*}
Die Fouriertransformierte einer Funktion geht aus der Laplacetransformierten durch ersetzen von $s=2\pi if$ hervor.\\
Eigenschaften:
\begin{itemize}
	\item \textbf{Linearität}:\\
			Eine Addition von Funktionen im Zeitbereich entspricht einer Addition der Laplacetransformierten im Bildbereich.\\
			Eine Multiplikation einer Funktion mit einem konstanten Faktor im Zeitbereich entspricht der Multiplikation mit demselben Faktor im Bildbereich.
			\begin{eqnarray*}
				f_1(t),f_2(t) & \laplace & F_1(s),F_2(s)\\
				\downarrow & & \downarrow \\
				C_1f_1(t)+C_2f_2(t) & \laplace & C_1F_1(s)+C_2F_2(s)
			\end{eqnarray*}
	\item \textbf{Ähnlichkeit}:\\
			Eine Ersetzung von $t$ durch $at$ der Funktion $f$ im Zeitbereich entspricht der Ersetzung von $s$ durch $\frac{s}{a}$im Bildbereich und einer Division der Laplacetransformierten durch $a$.
			\begin{eqnarray*}
				f(t) & \laplace & F(s)\\
				\downarrow & & \downarrow \\
				f(at) & \laplace & \frac{1}{a} F\left(\frac{s}{a}\right)
			\end{eqnarray*}
	\item \textbf{Verschiebung im Bildbereich/Dämpfungssatz}:\\
			Eine Verschiebung der Funktion $F$ um $s_0$ im Bildbereich entspricht der Multiplikation mit $e^{-s_0 t}$ im Zeitbereich.
			\begin{eqnarray*}
				F(s) & \Laplace & f(t)\\
				\downarrow & & \downarrow \\
				F(s-s_0) & \Laplace & e^{-s_0t}f(t)
			\end{eqnarray*}
\end{itemize}
\subsection{Differenziation und Integration}
\begin{itemize}
	\item \textbf{Differenziation im Zeitbereich}:\\
			\begin{eqnarray*}
				f(t) & \laplace & F(s)\\
				\downarrow & & \downarrow \\
				f'(t) & \laplace & s F\left(s\right) -f(0)
			\end{eqnarray*}
	\item \textbf{Höhere Ableitungen im Zeitbereich}:\\
			\begin{eqnarray*}
				f'(t) & \laplace & s F\left(s\right) -f(0)\\
				f''(t) & \laplace & s^2 F\left(s\right) -sf(0)-f'(0)\\
				... & & \\
				f^{(n)}(t) & \laplace & s^nF(s)-s^{n-1}f(0)- \dotsm -sf^{n-2}(0) - f^{n-1}(0)
			\end{eqnarray*}
	\item \textbf{Integration im Zeitbereich}:\\
		 	\begin{eqnarray*}
				f(t) & \laplace & F(s)\\
				\downarrow & & \downarrow \\
				\int_0^tf(\tau)d\tau & \laplace & \frac{1}{s} F\left(s\right)
			\end{eqnarray*}
	\item \textbf{Integration im Bildbereich}:\\
		 	\begin{eqnarray*}
				F(s) & \Laplace & f(t)\\
				\downarrow & & \downarrow \\
				\int_s^\infty F(u)du & \Laplace & \frac{1}{t} f\left(s\right)
			\end{eqnarray*}
	\item \textbf{Faltung im Zeitbereich}:\\
			\begin{eqnarray*}
				f_1(t),f_2(t) & \laplace & F_1(s),F_2(s)\\
				\downarrow & & \downarrow \\
				f_1(t)\star f_2(t) & \laplace & F_1(s)\cdot F_2(s)
			\end{eqnarray*}
\end{itemize}
\subsection{Rücktransformation}
Die Rücktrnsformation geschieht durch eine Korrespondenztabelle.
\subsection{Lösung von gewöhnlichen DGL mit Laplacetransformation}
\begin{itemize}
	\item Transformation der Differenzialgleichung in den Bildbereich.
	\item Lösung der algebraischen Gleichung im Bildbreich.
	\item Rücktransformation mittels Korrespondenztabelle.
\end{itemize}
\begin{eqnarray*}
	\text{Differentialgleichung} & \laplace & \text{Algebraische Gleichung}\\
	\downarrow & & \downarrow \\
	\text{Lösung Zeitbereich} & \laplace & \text{Lösung Bildbereich}
\end{eqnarray*}
\section{Zusatz}
\subsection{Ableitung}
\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		 & Funktion & Ableitung\\\hline\hline
		Fakorregel & $y = C \cdot f(x)$ & $y' = C\cdot f'(x)$\\\hline
		Produktregel & $y=u(x)\cdot v(x)$ & $y'=u'(x)\cdot v(x)+u(x)\cdot v'(x)$\\\hline
		Quotientenregel & $y=\frac{U(x)}{v(x)}$ & $y'=\frac{u'(x)\cdot v(x)-u(x)\cdot v'(x)}{\left[v(x)\right]^2}$\\\hline
		Kettenregel & $y=F(u(x))$ & $y'=\frac{dy}{dx}=\frac{dy}{du}\cdot\frac{du}{dx}$\\\hline
	\end{tabular}	
\end{table}
\subsection{Integration}
\begin{equation*}
	\int\limits_{a}^{b}f(x) dx = [F(x)]_a^b = F(b) - F(a)
\end{equation*}
\begin{equation*}
	\int\limits_{a}^{b}f(x) + g(x) dx = \int\limits_{a}^{b}f(x) dx + \int\limits_{a}^{b}g(x) dx
\end{equation*}
\begin{equation*}
	\int\limits_{a}^{b}c\cdot f(x) dx = c\cdot \int\limits_{a}^{b}f(x) dx
\end{equation*}

\subsection{Partielle Integration}
\begin{equation*}
	u(x)\cdot v(x) = \int u'(x) \cdot v(x) dx + \int u(x) \cdot v'(x) dx
\end{equation*}
\subsection{Stammfunktionen}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c||c|c|}
		\hline
		$F(x)$ & $f(x)$ & $F(x)$ & $f(x)$\\\hline\hline
		$x^n$ & $n\cdot n^{n-1}$ & $sin(x)$ & $cos(x)$\\\hline
		$\frac{1}{x}$ & $\frac{1}{-x^2}$ & $cos(x)$ & $-sin(x)$\\\hline
		$\sqrt{x}$ & $\frac{1}{2\sqrt{x}}$ & $sin^2(x)$ & $2\cdot cos(x)\cdot sin(x)$\\\hline
		$e^x$ & $e^x$ & $cos^2(x)$ & $-2\cdot cos(x)\cdot sin(x)$  \\\hline
		$ln(x)$ & $\frac{1}{x}$ & $tan(x)$ & $\frac{1}{cos^2(x)}$\\\hline
	\end{tabular}
\end{table}
\subsection{Partialbruchzerlegung}
\begin{equation*}
	\frac{19}{42}=\frac{19}{2\cdot3\cdot7}=\frac{A}{2}+\frac{B}{3}+\frac{C}{7}\stackrel{\text{Hauptnenner}}{=}\frac{21A+14B+6C}{2\cdot3\cdot7}\stackrel{A=1,B=-1,C=2}{=}\frac{1}{2}-\frac{1}{3}+\frac{2}{7}
\end{equation*}
\subsection{Gerade/Ungerade Funktionen}
\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Gerade Funktion} & \textbf{Ungerade Funktion}\\
		\hline\hline
		Funktion ist achsensymmetrisch zur y-Achse & Funktion ist punktsymmetrisch zum Ursprung\\\hline
		Summe ist wieder gerade & Summe ist wieder ungerade \\\hline
		Produkt ist wieder gerade & Produkt ist gerade!\\\hline
		Quotient ist wieder gerade & Quotient ist gerade!\\\hline
		Ableitung ist ungerade! & Ableitung ist gerade!\\\hline
		Fourier-Reihe enthält nur Kosinus-Terme & Fourier-Reihe enthält nur Sinus-Terme\\\hline
	\end{tabular}
\end{table}
\subsection{Eulersche Formel}
\begin{equation*}
	e^{ix}=cos(x)+isin(x)
\end{equation*}
\newpage
\subsection{Standartnormalverteilungstabelle}
\begin{table}[H]
	\begin{adjustbox}{width=\columnwidth,center}
	\begin{tabular}{|>{\bfseries}x||l|l|l|l|l|l|l|l|l|l|}
	\hline
	\rowcolor{LightCyan}
		z & \textbf{0} & \textbf{0,01} & \textbf{0,02} & \textbf{0,03} & \textbf{0,04} & \textbf{0,05} & \textbf{0,06} & \textbf{0,07} & \textbf{0,08} & \textbf{0,09}\\\hline\hline
	0,0 & 0,50000 & 0,50399 & 0,50798 & 0,51197 & 0,51595 & 0,51994 & 0,52392 & 0,52790 & 0,53188 & 0,53586\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,1 & 0,53983 & 0,54380 & 0,54776 & 0,55172 & 0,55567 & 0,55962 & 0,56356 & 0,56749 & 0,57142 & 0,57535\\\hline
	0,2 & 0,57926 & 0,58317 & 0,58706 & 0,59095 & 0,59483 & 0,59871 & 0,60257 & 0,60642 & 0,61026 & 0,61409\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,3 & 0,61791 & 0,62172 & 0,62552 & 0,62930 & 0,63307 & 0,63683 & 0,64058 & 0,64431 & 0,64803 & 0,65173\\\hline
	0,4 & 0,65542 & 0,65910 & 0,66276 & 0,66640 & 0,67003 & 0,67364 & 0,67724 & 0,68082 & 0,68439 & 0,68793\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,5 & 0,69146 & 0,69497 & 0,69847 & 0,70194 & 0,70540 & 0,70884 & 0,71226 & 0,71566 & 0,71904 & 0,72240\\\hline\hline
	0,6 & 0,72575 & 0,72907 & 0,73237 & 0,73565 & 0,73891 & 0,74215 & 0,74537 & 0,74857 & 0,75175 & 0,75490\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,7 & 0,75804 & 0,76115 & 0,76424 & 0,76730 & 0,77035 & 0,77337 & 0,77637 & 0,77935 & 0,78230 & 0,78524\\\hline
	0,8 & 0,78814 & 0,79103 & 0,79389 & 0,79673 & 0,79955 & 0,80234 & 0,80511 & 0,80785 & 0,81057 & 0,81327\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,9 & 0,81594 & 0,81859 & 0,82121 & 0,82381 & 0,82639 & 0,82894 & 0,83147 & 0,83398 & 0,83646 & 0,83891\\\hline
	1,0 & 0,84134 & 0,84375 & 0,84614 & 0,84849 & 0,85083 & 0,85314 & 0,85543 & 0,85769 & 0,85993 & 0,86214\\\hline\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,1 & 0,86433 & 0,86650 & 0,86864 & 0,87076 & 0,87286 & 0,87493 & 0,87698 & 0,87900 & 0,88100 & 0,88298\\\hline
	1,2 & 0,88493 & 0,88686 & 0,88877 & 0,89065 & 0,89251 & 0,89435 & 0,89617 & 0,89796 & 0,89973 & 0,90147\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,3 & 0,90320 & 0,90490 & 0,90658 & 0,90824 & 0,90988 & 0,91149 & 0,91309 & 0,91466 & 0,91621 & 0,91774\\\hline
	1,4 & 0,91924 & 0,92073 & 0,92220 & 0,92364 & 0,92507 & 0,92647 & 0,92785 & 0,92922 & 0,93056 & 0,93189\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,5 & 0,93319 & 0,93448 & 0,93574 & 0,93699 & 0,93822 & 0,93943 & 0,94062 & 0,94179 & 0,94295 & 0,94408\\\hline\hline
	1,6 & 0,94520 & 0,94630 & 0,94738 & 0,94845 & \cellcolor{LightRed}0,94950 & 0,95053 & 0,95154 & 0,95254 & 0,95352 & 0,95449\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,7 & 0,95543 & 0,95637 & 0,95728 & 0,95818 & 0,95907 & 0,95994 & 0,96080 & 0,96164 & 0,96246 & 0,96327\\\hline
	1,8 & 0,96407 & 0,96485 & 0,96562 & 0,96638 & 0,96712 & 0,96784 & 0,96856 & 0,96926 & 0,96995 & 0,97062\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,9 & 0,97128 & 0,97193 & 0,97257 & 0,97320 & 0,97381 & 0,97441 & \cellcolor{LightRed}0,97500 & 0,97558 & 0,97615 & 0,97670\\\hline
	2,0 & 0,97725 & 0,97778 & 0,97831 & 0,97882 & 0,97932 & 0,97982 & 0,98030 & 0,98077 & 0,98124 & 0,98169\\\hline\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,1 & 0,98214 & 0,98257 & 0,98300 & 0,98341 & 0,98382 & 0,98422 & 0,98461 & 0,98500 & 0,98537 & 0,98574\\\hline
	2,2 & 0,98610 & 0,98645 & 0,98679 & 0,98713 & 0,98745 & 0,98778 & 0,98809 & 0,98840 & 0,98870 & 0,98899\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,3 & 0,98928 & 0,98956 & 0,98983 & \cellcolor{LightRed}0,99010 & 0,99036 & 0,99061 & 0,99086 & 0,99111 & 0,99134 & 0,99158\\\hline
	2,4 & 0,99180 & 0,99202 & 0,99224 & 0,99245 & 0,99266 & 0,99286 & 0,99305 & 0,99324 & 0,99343 & 0,99361\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,5 & 0,99379 & 0,99396 & 0,99413 & 0,99430 & 0,99446 & 0,99461 & 0,99477 & 0,99492 & 0,99506 & 0,99520\\\hline\hline
	2,6 & 0,99534 & 0,99547 & 0,99560 & 0,99573 & 0,99585 & 0,99598 & 0,99609 & 0,99621 & 0,99632 & 0,99643\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,7 & 0,99653 & 0,99664 & 0,99674 & 0,99683 & 0,99693 & 0,99702 & 0,99711 & 0,99720 & 0,99728 & 0,99736\\\hline
	2,8 & 0,99744 & 0,99752 & 0,99760 & 0,99767 & 0,99774 & 0,99781 & 0,99788 & 0,99795 & 0,99801 & 0,99807\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,9 & 0,99813 & 0,99819 & 0,99825 & 0,99831 & 0,99836 & 0,99841 & 0,99846 & 0,99851 & 0,99856 & 0,99861\\\hline
	3,0 & 0,99865 & 0,99869 & 0,99874 & 0,99878 & 0,99882 & 0,99886 & 0,99889 & 0,99893 & 0,99896 & 0,99900\\\hline\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,1 & 0,99903 & 0,99906 & 0,99910 & 0,99913 & 0,99916 & 0,99918 & 0,99921 & 0,99924 & 0,99926 & 0,99929\\\hline
	3,2 & 0,99931 & 0,99934 & 0,99936 & 0,99938 & 0,99940 & 0,99942 & 0,99944 & 0,99946 & 0,99948 & 0,99950\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,3 & 0,99952 & 0,99953 & 0,99955 & 0,99957 & 0,99958 & 0,99960 & 0,99961 & 0,99962 & 0,99964 & 0,99965\\\hline
	3,4 & 0,99966 & 0,99968 & 0,99969 & 0,99970 & 0,99971 & 0,99972 & 0,99973 & 0,99974 & 0,99975 & 0,99976\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,5 & 0,99977 & 0,99978 & 0,99978 & 0,99979 & 0,99980 & 0,99981 & 0,99981 & 0,99982 & 0,99983 & 0,99983\\\hline\hline
	3,6 & 0,99984 & 0,99985 & 0,99985 & 0,99986 & 0,99986 & 0,99987 & 0,99987 & 0,99988 & 0,99988 & 0,99989\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,7 & 0,99989 & 0,99990 & 0,99990 & 0,99990 & 0,99991 & 0,99991 & 0,99992 & 0,99992 & 0,99992 & 0,99992\\\hline
	3,8 & 0,99993 & 0,99993 & 0,99993 & 0,99994 & 0,99994 & 0,99994 & 0,99994 & 0,99995 & 0,99995 & 0,99995\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,9 & 0,99995 & 0,99995 & 0,99996 & 0,99996 & 0,99996 & 0,99996 & 0,99996 & 0,99996 & 0,99997 & 0,99997\\\hline
	4,0 & 0,99997 & 0,99997 & 0,99997 & 0,99997 & 0,99997 & 0,99997 & 0,99998 & 0,99998 & 0,99998 & 0,99998\\\hline
	\end{tabular}
	\end{adjustbox}
\end{table}
\newpage
\subsection{Korrespondenztabelle}
\begin{tabular}{|r|c|c||r|c|c|}
	\hline
	Nr. & Bildfunktion $F(s)$ & Zeitfunktion $f(t)$ & Nr. & Bildfunktion $F(s)$ & Zeitfunktion $f(t)$ \\\hline\hline
	1 & $1$ & $\delta(t)$ & 17 & $\frac{a}{s^2+a^2}$ & $\sin at$\\
	2 & $\frac{1}{s}$ & $\sigma(t)$ & 18 & $\frac{s}{s^2+a^2}$ & $\cos at$\\
	3 & $\frac{1}{s^2}$ & $t$ & 19 & $\frac{a}{s^2-a^2}$ & $\sinh at$\\
	4 & $\frac{n!}{s^n}$ & $t^n$ & 20 & $\frac{s}{s^2-a^2}$ & $\cosh at$\\\hline
	5 & $\frac{1}{s-a}$ & $e^{at}$ & 21 & $\frac{a}{(s-b)^2+a^2}$ & $e^{bt}\sin at$\\
	6 & $\frac{1}{(s-a)^2}$ & $te^{at}$ & 22 & $\frac{s-b}{(s-b)^2+a^2}$ & $e^{bt}\cos at$\\
	7 & $\frac{a}{s(s-a)}$ & $e^{at}-1$ & 23 & $\frac{a}{(s-b)^2-a^2}$ & $e^{bt}\sinh at$\\
	8 & $\frac{a-b}{(s-a)(s-b)}$ & $e^{at}-e^{bt}$ & 24 & $\frac{s-b}{(s-b)^2-a^2}$ & $e^{bt}\cosh at$\\\hline
	9 & $\frac{a}{1+as}$ & $e^{-\frac{t}{a}}$ & 25 & $\frac{2as}{(s^2+a^2)^2}$ & $t\sin at$\\
	10 & $\frac{a^2}{(1+as)^2}$ & $te^{-\frac{t}{a}}$ & 26 & $\frac{s^2-a^2}{(s^2+a^2)^2}$ & $t\cos at$\\
	11 & $\frac{1}{s(1+as)}$ & $1-e^{-\frac{t}{a}}$ & 27 & $\frac{2as}{(s^2-a^2)^2}$ & $t\sinh at$\\
	12 & $\frac{a-b}{(1+as)(1+bs)}$ & $e^{-\frac{t}{a}}-e^{-\frac{t}{b}}$ & 28 & $\frac{s^2+a^2}{(s^2-a^2)^2}$ & $t\cosh at$\\\hline
	13 & $\frac{s}{(s-a)^2}$ & $(1+at)e^{at}$ & 29 & $\frac{2}{(s-a)^3}$ & $t^2e^{at}$\\
	14 & $\frac{(a-b)s}{(s-a)(s-b)}$ & $ae^{at}+be^{bt}$ & 30 & $\frac{2s}{(s-a)^3}$ & $(at^2+2t)e^{at}$\\
	15 & $\frac{a^3s}{(1+as)^2}$ & $(a-t)e^{-\frac{t}{a}}$ & 31 & $\frac{2s^2}{(s-a)^3}$ & $(a^2t^2+4at+2)e^{at}$\\
	16 & $\frac{ab(a-b)s}{(1+as)(1+bs)}$ & $ae^{-\frac{t}{b}}-be^{-\frac{t}{a}}$ & 32 & $\frac{a^2}{s^2(s-a)}$ & $e^{at}-at-1$\\\hline
\end{tabular}
\subsection{Mathematische Symbole für Mengen}
\begin{tabular}{|p{0.1\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}|}
	\hline
	Symbol & Verwendung & Bedeutung\\\hline\hline
	$\in$ & $\omega\quad\in\quad\Omega$ & Element ($\omega$ ist in $\Omega$ enthalten) \\\hline
	$\cap$ & $A\quad\cap\quad B$ & Disjunkt (Kein Teil von A ist ein Teil von B) \\\hline
	$\cup$ & $A\quad\cup\quad B$ & Kunjunktion (Ein Teil von A ist ein Teil von B)\\\hline
	$\subseteq$ & $A\quad\subseteq\quad B$ & Teilmenge (A ist eine Teilmenge von B)\\\hline
	$\setminus$ & $A\quad\setminus\quad B$ & Differenz (Differenz der mengen A und B)\\\hline
	$^\mathrm{C}$ & $A^\mathrm{C}$ & Komplement (Differenz des Universums (kann eine\newline größere Menge sein) und der Teilmenge)\\\hline
	$\mathbb{N}$ & Natürliche Zahlen & Positive Ganze Zahlen ohne Null (1,2,3,...)\\\hline
	$\mathbb{Z}$ & Ganze Zahlen & Ganze Zahlen (...,-2,-1,0,1,2,...)\\\hline
	$\mathbb{Q}$ & Rationale Zahlen & $z\cdot \frac{1}{x}$ mit $z,x \in \mathbb{Z}$\\\hline
	$\mathbb{R}$ & Reelle Zahlen & Erweiterung der Rationalen Zahlen durch diejenigen\newline Zahlen welche sich nicht durch Brüche darstellen\newline lassen (z.B.$\sqrt{2}, \pi$)\\\hline
	$\mathbb{C}$ & Komplexe Zahlen & $a+bi$ mit $a,b\in \mathbb{R}$ und $i^2 = -1$\\\hline
\end{tabular}

%\newpage
%\pagenumbering{Alph}
%\section{Anhang}
%\printbibliography[heading=subbibnumbered]
\end{document}