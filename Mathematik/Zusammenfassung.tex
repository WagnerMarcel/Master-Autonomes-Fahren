\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{pdflscape}
\usepackage[margin=2cm, left=3cm]{geometry}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tocbasic}
\usepackage{fancyhdr}
\usepackage{float} 
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[english, ngerman]{babel}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{color, colortbl}
\usepackage{array}
\usepackage{diagbox}
\usepackage{trfsigns}
\usepackage[
    backend=biber,		
    bibwarn=true,
    bibencoding=utf8,	% wenn .bib in utf8, sonst ascii
    sortlocale=de_DE,
    style=numeric-comp,
]{biblatex}
\usepackage{csquotes}

\addbibresource{bibliographie.bib}

\selectlanguage{ngerman}

\hypersetup{%
    colorlinks=true, 		% Aktivieren von farbigen Links im Dokument
    linkcolor=black, 	    % Farbe festlegen
    citecolor=black,
    filecolor=black,
    menucolor=black,
    urlcolor=black,
    linktoc=all,            % Seitenzahlen und Text klickbar
    bookmarksnumbered=true 	% Überschriftsnummerierung im PDF Inhalt anzeigen.
}

\newcommand\todo[1]{\textit{\textcolor{red}{\\TODO: #1}}}

\DeclareNewTOC[%
 forcenames,
 type=formel,
 name={Formel},%
 listname={Formelverzeichnis}
]{for}
\newcommand*{\formelentry}[1]{%
     \addcontentsline{for}{formel}{\protect\numberline{\theequation} #1}%
    }



\title{Master Autonomes Fahren - Mathematik Zusammenfassung}
\author{Marcel Wagner}
\date{\today}

\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcolumntype{x}{>{\columncolor{LightCyan}}l}


\begin{document}
\pagenumbering{Roman}
\maketitle
\tableofcontents
%\newpage
%\listoffigures
%\listoffigures
%\listofformels
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Mathematische Symbole}
\subsection{Mengen}
\begin{tabular}{p{0.1\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}}
	Symbol & Verwendung & Bedeutung\\
	\hline
	$\in$ & $\omega\quad\in\quad\Omega$ & Element ($\omega$ ist in $\Omega$ enthalten) \\
	$\cap$ & $A\quad\cap\quad B$ & Disjunkt (Kein Teil von A ist ein Teil von B) \\
	$\cup$ & $A\quad\cup\quad B$ & Kunjunktion (Ein Teil von A ist ein Teil von B)\\
	$\subseteq$ & $A\quad\subseteq\quad B$ & Teilmenge (A ist eine Teilmenge von B)\\
	$\setminus$ & $A\quad\setminus\quad B$ & Differenz (Differenz der mengen A und B)\\
	$^\mathrm{C}$ & $A^\mathrm{C}$ & Komplement (Differenz des Universums (kann eine\newline größere Menge sein) und der Teilmenge)\\
	$\mathbb{N}$ & Natürliche Zahlen & Positive Ganze Zahlen ohne Null (1,2,3,...)\\
	$\mathbb{Z}$ & Ganze Zahlen & Ganze Zahlen (...,-2,-1,0,1,2,...)\\
	$\mathbb{Q}$ & Rationale Zahlen & $z\cdot \frac{1}{x}$ mit $z,x \in \mathbb{Z}$\\
	$\mathbb{R}$ & Reelle Zahlen & Erweiterung der Rationalen Zahlen durch diejenigen\newline Zahlen welche sich nicht durch Brüche darstellen\newline lassen (z.B.$\sqrt{2}, \pi$)\\
	$\mathbb{C}$ & Komplexe Zahlen & $a+bi$ mit $a,b\in \mathbb{R}$ und $i^2 = -1$\\
\end{tabular}

\section{Statistik}
\subsection{Arithmetisches Mittel}
\begin{equation*}
	\bar{x} := \frac{x_1 + ... + x_n}{n} = \frac{1}{n}\sum_{i=1}^{n}{x_i}
\end{equation*}
\subsection{Mittlerer Abstand}
Der mittlere Abstand wird nicht sehr häufig verwendet, da das Rechnen mit Beträgen sehr mühsam ist. Die Varianz (durchschnittliche quadratische Abweichung) eignet sich besser.
\begin{equation*}
	\frac{1}{n}\sum_{i=1}^{n}{|x_i-\bar{x}|}
\end{equation*}
\subsection{Varianz}
\begin{equation*}
	s_x^2= \frac{1}{n-1}\sum_{i=1}^n{(x_i-\bar{x})^2}
\end{equation*}
\subsection{Standartabweichung}
\begin{equation*}
	s_x = \sqrt{s_x^2}
\end{equation*}
\subsection{Kovarianz}
\begin{equation*}
	y_{xy}:=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})
\end{equation*}
\subsection{Korrelationskoeffizient}
\begin{equation*}
	r_{xy}:=\frac{s_{xy}}{s_x\cdot s_y}
\end{equation*}
\subsection{Regressionsgerade}
\begin{equation*}
	y = a + bx
\end{equation*}
\begin{equation*}
	b = \frac{s_{xy}}{s_x^2}
\end{equation*}
\begin{equation*}
	a = \bar{y} - b\bar{x}
\end{equation*}
\subsection{Bestimmtheitsmaß}
\begin{equation*}
	R^2 = \frac{\sum_{i=1}^n (\hat{y}_i-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i-\bar{y})^2}
\end{equation*}
mit Arithmetischem Mittel $\bar{y}$ und Ausgleichsgerade $\hat{y}_i = y(x_i) = a+bx_i$.
\begin{equation*}
	R^2 = r_{xy}^2
\end{equation*}

\section{Wahrscheinlichkeitsrechnung}
\subsection{Fakultät}
\begin{equation*}
	n! = n \cdot (n-1) \cdot \dotsc \cdot 2 \cdot 1 
\end{equation*}
\subsection{Binomialkoeffizient}
\begin{equation*}
	{n \choose k} = \frac{n!}{k!\cdot(n-k)!}
\end{equation*}
\subsection{Kugeln Ziehen}
\begin{tabular}{c|c|c}
	 & mit Reihenfolge & ohne Reihenfolge 
	 \\\hline
	 mit Zurücklegen & $n^k$ & ${n+k-1 \choose k}$
	 \\\hline
	 ohne Zurücklegen & $\frac{n!}{(n-k)!}$ & ${n \choose k}$
\end{tabular}
\subsection{Menge}
Unter einer Menge verstehen wir die Zusammenfassung unterscheidbarer Elemente zu einer Gesamtheit.
\subsubsection{Gleichheit}
$A = B:\Leftrightarrow$ $A$ und $B$ besitzen die gleichen Elemente.
\subsubsection{Teilmenge}
$A \subset B:\Leftrightarrow$ wenn alle Elemente von $A$ auch in $B$ sind, dann ist $A$ eine Teilmenge von $B$ oder auch $B$ die Obermenge von $A$.\\
Jede Menge ist Teilmenge von sich selbst. 
\subsubsection{Potenzmenge}
Die Potenzmenge $\mathcal{P}(X)$ ist eine Menge welche aus allen Teilmengen von $U \subseteq X$ besteht.
\subsubsection{Mächtigkeit}
$|A|:=$ Zahl der Elemente von A.
\subsubsection{Vereinigung}
$A\cup B:=$ Menge aus allen Elementen welche in $A$ oder in $B$ oder in beiden enthalten sind.
\subsubsection{Schnitt}
$A\cap B:=$ Menge aus allen Elementen welche in $A$ und in $B$ enthalten sind.
\subsubsection{Differenz}
$A\setminus B:=$ Menge aus allen Elementen welche zu $A$ aber \textbf{nicht} zu $B$ gehören.
\subsubsection{Komplement}
$A^C:=$ Menge aus allen Elementen welche \textbf{nicht} zu $A$ gehören.
\subsubsection{Kartesisches Produkt}
\begin{equation*}
	A \times B:={(a, b):a\in A, b\in B}
\end{equation*}
\subsubsection{$\sigma$-Algebra}
Eine Teilmenge einer Potenzmenge (Menge von Teilmengen, $\mathcal{A}\subseteq\mathcal{P}(\Omega)$) heißt $\sigma$-Algebra wenn sie folgende Bedingungen erfüllt:
\begin{itemize}
	\item Die Teilmenge $\mathcal{A}$ der Potenzmenge $\mathcal{P}(\Omega)$ enthält die Grundmenge $\Omega$.
	\item Das Komplement $A^\mathrm{C}$ eines Elements der Teilmenge $A \in\mathcal{A}$ ist gleich der Differenz aus Grundmenge und Element $A^\mathrm{C} := \Omega\setminus A$. Stabilität des Komplements.
	\item Sind die Mengen in der Teilmenge der Potenzmenge $A_1,A_2,A_3,... \in \mathcal{A}$ enthalten, so ist auch die Vereinigung aller Mengen in der Teilmenge der Potenzmenge enthalten $\bigcup\limits_{n\epsilon \mathbb{N}}A_n \in \mathcal{A}$
	\item Alle vorangegangenen Mengenoperationen können auf die Teilmengen angewendet werden. 
\end{itemize}

\subsection{Zufallsexperiment}
\begin{itemize}
	\item Genau festgelegte Bedingungen
	\item Zufälliger Ausgang
	\item Beliebig oft wiederholbar
	\item Ein Versuch bezeichnet einen Vorgang bei dem mehrere Ergebnisse (Elementarereignis) eintreten können
	\item Menge aller Elementarereignisse wird als Ergebnismenge (Ergebnisraum) $\Omega$ bezeichnet
\end{itemize}
\subsection{Ereignis}
\begin{itemize}
	\item Eine Teilmenge $A \subset \Omega$ heißt Ereignis
	\item $A = \emptyset$ unmögliches Ereignis
	\item $A = \Omega$ sicheres Ereignis
\end{itemize}
\subsubsection{Disjunkte Ereignisse}
Zwei ereignisse sind disjunkt (unvereinbar) wenn deren Schnitt gleich der leeren Menge ist $A \cap B = \emptyset$.
\subsubsection{Unabhängige Ereignisse}
Zwei Ereignisse heißen \textbf{unabhängig} wenn gilt: 
\begin{equation*}
	P(A\cap B) = P(A) \cdot P(B)
\end{equation*}
Sie heißen \textbf{abhängig} wenn sie nicht unabhängig sind.\\
Für unabhängige Ereignisse gilt:
\begin{equation*}
	P(A) = \frac{P(A\cap B)}{P(B)} \text{\quad bzw. \quad} P(B)=\frac{P(A\cap B)}{P(A)}
\end{equation*} 
\subsection{Axiome der Wahrscheinlichkeitsrechnung}
Die Funktion $P$ ordnet jedem Ereignis $A$ eine Wahrscheinlichkeit $P(A)$ zu.
\begin{enumerate}
	\item[(I)] Für jedes Ereignis $A\subset \Omega$ gilt $0\leq P(A) \leq 1$
	\item[(I')] Für das unmögliche Ereignis gilt $P(\emptyset) = 0$
	\item[(II)] Für das sichere Ereignis $\Omega$ gilt $P(\Omega) = 1$
	\item[(II')] Für ein Ereignis $A \subset \Omega$ gilt $P(A^C) = 1- P(A)$
	\item[(III)] Für disjunkte Ereignisse $A$ und $B$ gilt $P(A\cup B) = P(A)+P(B)$ 
	\item[(III')] Für zwei Ereignisse $A,B \subset \Omega$ gilt $P(A\cup B)= P(A)+P(B)-P(A\cap B)$ 
\end{enumerate}
\subsection{Laplace Experiment}
Endlich viele Elementarereignisse welche alle gleich wahrscheinlich sind.\\
Satz von Laplace:
\begin{equation*}
	P(A) = \frac{|A|}{|\Omega|}=\frac{\text{Anzahl der Elementarereignisse in $A$}}{\text{Anzahl aller möglichen Elementarereignisse}}
\end{equation*}
\subsection{Bedingte Wahrscheinlichkeit}
"Wahrscheinlichkeit von A gegeben B".
\begin{equation*}
	P(A|B) := \frac{P(A\cap B)}{P(B)}
\end{equation*}
Sind $A, B \subset \Omega$ \textbf{unabhängige} Ereignisse gilt:
\begin{equation*}
	P(A|B) = P(A)
\end{equation*}
Sind $A, B \subset \Omega$ \textbf{abhängige} Ereignisse gilt:
\begin{equation*}
	P(A|B) \neq P(A)
\end{equation*}
\subsubsection{Multiplikationssatz}
\begin{equation*}
	P(A\cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)
\end{equation*}
\subsubsection{Satz der totalen Wahrscheinlichkeit}
Der Ergebnisraum ist gegeben durch $\Omega = \bigcup\limits_{j=1}^\infty B_j$ mit $P(B_j)>0$ und alle $j$ sind paarweise Disjunkt $B_i \cap B_j = \emptyset$ für $i\neq j$
\begin{equation*}
	P(A) = \sum_{j=1}^\infty P(A|B_j) \cdot P(B_j)
\end{equation*}
Für den Spezialfall $\Omega = B\cup B^C$ gilt:
\begin{equation*}
	P(A) = P(B) \cdot P(A|B) + P(B^C) \cdot P(A|B^C)
\end{equation*}
\subsubsection{Satz von Bayes}
Besteht aus dem Multiplikationssatz \& der totalen Wahrscheinlichkeit:
\begin{equation*}
	P(A|B)=\frac{P(A)\cdot P(B|A)}{P(B)}= \frac{P(A)\cdot P(B|A)}{P(A)\cdot P(B|A) + P(A^C)\cdot P(B|A^C)}
\end{equation*}
\subsection{Zufallsvariablen}
Eine Zufallsvariable ist eine Abbildung des Ergebnisraums auf den reellen Zahlenraum $\Omega\longmapsto\mathbb{R}$. Die Zufallsvariable ordnet jedem Elementarereignis eine reelle Zahl zu.\\
Zwei Zufallsvariablen sind \textbf{unabhängig} wenn gilt:
\begin{equation*}
	P(X\in A, Y\in B) = P(X\in A) \cdot P(Y\in B) \text{\quad für alle\quad} A,B\subset \mathbb{R} 
\end{equation*}
Die Zufallsvariablen heißen \textbf{abhängig} wenn sie nicht unabhängig sind.
\subsubsection{Diskrete Zufallsvariable}
Die Zufallsvariable wird \textbf{diskret} genannt wenn sie nur endlich viele oder abzählbar unendlich viele Werte annimmt. Es gilt:
\begin{equation*}
	\sum\limits_{i=1}^\infty P(X=x_i)=1
\end{equation*}
\subsubsection{Wahrscheinlichkeitsfunktion}
Für die diskrete Zufallsvariable $X$ und ihre Ausprägungen lautet die Wahrscheinlichkeitsfunktion:
\begin{equation*}
	p_X(x):=
	\begin{cases}
		P(X=x_i) \text{, für } x = x_i \text{ mit Zählindex } i \in \mathbb{N}\\
		0 \text{, sonst}
	\end{cases}
\end{equation*}
\begin{equation*}
	\sum\limits_{x_i}p_X(x_i)=1=p(\Omega)
\end{equation*}
\subsubsection{Verteilungsfunktion diskreter Zufallsvariablen}
Für die diskrete Zufallsvariable $X$ und ihre Ausprägungen lautet die Verteilungsfunktion:
\begin{equation*}
	F_X(x):=P(X\leq x)=\sum_{x_i\leq x}P(X=x_i)=\sum_{x_i\leq x}p_X(x_i)
\end{equation*}
\subsubsection{Stetige Zufallsvariable}
Eine zuvallsvariable wird \textbf{stetig} genannt, wenn es eine nicht-negative Funktion $f_X\geq 0$ mit 
\begin{equation*}
	\int\limits_{-\infty}^\infty f_X(x) dx = 1
\end{equation*}
gibt, so dass für alle $a,b \in \mathbb{R} \cup \{-\infty, \infty\}$ mit $a\leq b$ gilt:
\begin{equation*}
	P(X\in [a,b]) = P(a\leq X \leq b) = \int\limits_a^b f_X(x)dx
\end{equation*}
$f_X$ wird als \textbf{Dichtefunktion} (Wahrscheinlichkeitsdichte) der Zufallsvariable $X$ bezeichnet.
Ihre Verteilungsfunktion $F_X$ lautet:
\begin{equation*}
	F_X(x):=P(X\leq x) = \int\limits_{-\infty}^x f_X(u)du
\end{equation*}
Außerdem gilt:
\begin{equation*}
	f_X = F'_X
\end{equation*}
Daraus folgt:
\begin{equation*}
	P(X\in [a,b]) = P(a\leq X \leq b) = \int\limits_a^b f_X(x)dx = F_X(b) - F_X(a)
\end{equation*}
\subsubsection{Symmetrische Zufallsvariable}
Eine Zufallsvariable $X$ wird \textbf{symmetrisch} genannt, wenn es eine Symmetrieachse $c \in \mathbb{R}$ gibt, so dass für alle $d\in\mathbb{R}$ gilt:
\begin{itemize}
	\item für diskrete Zufallsvariablen 
	\begin{equation*}
		P(X=c-d) = P(X=c+d)
	\end{equation*}
	\item für stetige Zufallsvariablen
	\begin{equation*}
		f_X(c-d)=f_X(c+d)
	\end{equation*}
\end{itemize}
\subsubsection{Mehrdimensionale Verteilungsfunktion}
Die \textbf{Verteilungsfunktion} einer zweidimensionalen Zufallsveriablen $Z = (X_1,...,X_n)$ wird definiert durch:
\begin{equation*}
	F_Z(x_1,...,y)=P(X_1\leq x_1,..., X_n\leq x_n).
\end{equation*}
\subsubsection{Rand-Verteilungsfunktion}
Als Rand-Verteilungsfunktion einer mehrdimensionalen Zufallsvariablen $Z=(X_1,...,X_n)$ wird diejenige Funktion bezeichnet welche lediglich eine dimension betrachtet.
\begin{equation*}
	F_{X_i}(x_i)=F_Z(\infty,...,\infty,x_i,\infty,...,\infty)
\end{equation*}
Für die zweidimensionale Rand-Verteilungsfunktion ($Z=(X,Y)$, $F_Z(x,y)$) gilt:
\begin{equation*}
	F_X(x)=F_Z(x,\infty) \text{ sowie } F_Y(y) = F_Z(\infty,y)
\end{equation*}
\subsubsection{Totale Wahrscheinlichkeit}
\begin{equation*}
	f_X(x) = \int f_{X,Y}(x,y)dy = \int f_Y(y) \cdot f_X(x|Y=y) dy
\end{equation*}
Mit dieser Formel lässt sich eine Rand-Dichte aus einer gemeinsamen Dichte bestimmen, dies wird als \textbf{Marginalisierung} bezeichnet.
\subsubsection{Erwartungswert einer Zufallsvariable}
Für eine diskrete Zufallsvariable mit $(x_i)_{i\in \mathbb{N}}$ Ausprägungen und Wahrscheinlichkeitsfunktion $p_X$ lautet der \textbf{Erwartungswert}:
\begin{equation*}
	E(X):=\sum\limits_{i=1}^\infty x_i\cdot p_X(x_i)
\end{equation*}
Für eine stetige Zufallsvariable mit Dichte $f_X$ lautet der Erwartungswert:
\begin{equation*}
	E(X):=\int\limits_{-\infty}^\infty x\cdot f_X(x) dx
\end{equation*}
\subsubsection{Transformationen von Zufallsvariablen}
\begin{itemize}
	\item Linearität:
	\begin{equation*}
		E(a\cdot X + b\cdot Y)=a\cdot E(X)+b\cdot E(Y)
	\end{equation*}
	\item Multiplikation:
	\begin{equation*}
		E(X\cdot Y) = E(X)\cdot E(Y)
	\end{equation*}
\end{itemize}
\subsubsection{Varianz einer Zufallsvariable}
Eine Zufallsvariable mit Erwartungswert $\mu = E(X)$ hat die \textbf{Varianz}:
\begin{equation*}
	\sigma^2(X):=E[(X-\mu)^2]=E(X^2)-\mu^2
\end{equation*}
Die Standartabweichung lautet:
\begin{equation*}
	\sigma(X) = \sqrt{\sigma^2(X)}
\end{equation*}
\subsubsection{Grenzwertsatz von Zufallsvariablen}
Für $X_1,...,X_n$ unabhängige und identisch verteilte Zufallsvariablen mit $E(X_i)=\mu$, $\sigma(X_i)=\sigma$ und $\bar{X}:=\frac{1}{n}(X_1+...+X_n)$ gilt:
\begin{gather*}
	E(\bar{X})=\mu \\
	\sigma^2(\bar{X})=\frac{\sigma^2}{n}\\
	\sigma(X)=\frac{\sigma}{\sqrt{n}}
\end{gather*}
\subsection{Quantil}
Bezeichnet das kleinste $x$ mit $F_X(x)\geq p$. \\
Spezielle Quantile sind:
\begin{itemize}
	\item $x_{0.5}$ Median
	\item $x_{0.25}, x_{0.5}, x_{0.75}$ erstes, zweites und drittes Quantil
	\item $x_{0.01}, x_{0.02}, x_{0.03}, ...$ erstes, zweites, drittes, ... Perzentil
\end{itemize}
\subsection{Diskrete Verteilungen}
\subsubsection{Bernoulli-Verteilung}
Eine Zufallsvariable wird \textbf{Bernoulli-verteilt} genannt, wenn sie nur zwei mögliche Ausprägungen (z.B. 0 \& 1) hat.  Ihre Wahrscheinlichkeit lautet:
\begin{equation*}
	p:=P(X=1) \text{ und } q:=1-p=P(X=0)
\end{equation*}
Außerdem gilt:
\begin{gather*}
	E(X)=p\\
	\sigma^2(X)=p\cdot q=p\cdot (1-p)\\
	\sigma(X)=\sqrt{p\cdot q}=\sqrt{p\cdot (1-p)}
\end{gather*}
\subsubsection{Binomialverteilung}
Eine \textbf{Binomialverteilung} $X$ bezeichnet die Anzahl der Erfolge bei $n$ identischen unabhängigen Bernoulli-Experimenten $X \sim B(n;p)$.  
\begin{gather*}
	B(n;p)(k) := p_X(k) = P(X=k):= {n \choose k}p^k\cdot (1-p)^{n-k} \text{ für } k=0,1,...,n \\
	 B(n;p)(k) := \frac{n!}{k!\cdot(n-k)!}p^k\cdot (1-p)^{n-k}
\end{gather*}
Weiter gilt:
\begin{gather*}
	E(X)=n\cdot p\\
	\sigma^2(X)=n\cdot p \cdot (1-p)\\
	\sigma(X)=\sqrt{n\cdot p\cdot (1-p)}
\end{gather*}
Eine Binmialverteilung ist für $p=0,0.5,1$ symmetrisch. Für alle anderen Werte ist sie nicht symmetrisch.\\
Aufgrund der Symmetrie gilt zudem:
\begin{equation*}
	B(n;p)(k) = B(n;1-p)(n-k) \text{ mit } n\in \mathbb{N}, p\in [0,1]
\end{equation*}
Für zwei Binominalverteilungen $X\sim B(n_1,p)$ und $Y\sim B(n_2,p)$ gilt, dass deren Summe wieder Binomialverteilt ist:
\begin{equation*}
	X+Y\sim B(n_1+n_2,p)
\end{equation*}
Eine Binomialverteilung $X$ mit seltenen Ereignissen $(p\approx0, N\gg 0)$ wird \textbf{Poissonverteilung} genannt $X\sim Po(\lambda)$. Sie wird approximiert durch:
\begin{equation*}
	p_X(k)=P(X=k):=\frac{\lambda^k}{k!}\cdot e^{-\lambda} \text{ für } k=0,1,2,3,... \text{ und mit } \lambda:=E(X)
\end{equation*}
\subsection{Stetige Verteilungen}
\subsubsection{Gleichverteilung}
Die \textbf{Gleichverteilung} $X$ auf $[a,b]\subset \mathbb{R}$ ($X\sim U([a,b])$) besitzt folgende Dichte:
\begin{equation*}
	f_X(x)=\begin{cases}
		\frac{1}{b-a} \text{, für} x\in [a,b] = \int_a^b\frac{1}{b-a}dx=\frac{b}{b-a}-\frac{a}{b-a}=1\\
		0\text{, sonst}
	\end{cases}
\end{equation*}
Es gilt:
\begin{gather*}
	E(X)=\frac{a+b}{2}\\
	\sigma^2 =\frac{(b-a)^2}{12}\\
	\sigma = \frac{b-a}{\sqrt{12}}
\end{gather*}
\subsubsection{Inversionsmethode}
Es sei $X$ eine Zufallsvariable und $F_X$ ihre Verteilungsfunktion.\\
Die Funktion $F_X^{-1}$ ist die inverse Verteilungsfunktion (\textbf{Quantil-Funktion}):
\begin{equation*}
	F_X^{-1}(u):= \inf\{x\in\mathbb{R}|F(x)\geq u\}
\end{equation*}
Bedeutet, die inverse Verteilungsfunktion liefert das kleinste $x$ welches in der Verteilungsfunktion den Funktionswert $u$ überschreitet.\\
Für eine gleichverteilte Zufallsvariable (das bedeutet alle Zahlen von 0 bis 1 kommen gleich häufig vor) $U\sim U([0,1])$ gilt:
\begin{equation*}
	X:=F_X^{-1}(U) \text{ hat die Verteilungsfunktion } F_X
\end{equation*}
Erklärung: U ist von 0 bis 1 gleichverteilt (alle Zahlen (x-Achse) kommen gleich häufig vor). Nun wird jeder Wert der Zufallsvariable U in die inverse Verteilungsfunktion eingesetzt. Dadurch wird jetzt die Funktion $F_X$ nachgebildet, da immer das kleinste x der Verteilungsfunktion für den Wert von U zurückgegeben wird.
\subsection{Normalverteilung}
Die \textbf{Normalverteilung} $X$ ($X\sim N(\mu , \sigma^2)$) mit den Parametern $\mu\in\mathbb{R}$ und $\sigma\in\mathbb{R}^+$ besitzt folgende Dichte und wird auch \textbf{Gaußsche Glockenkurve} genannt:
\begin{equation*}
	f_X(x)=\frac{1}{\sigma\sqrt{2\pi}}\cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end{equation*}
Die Gaußsche Glockenkurve besitzt an der Stelle $\mu$ ein Maximum, sowie zwei Wendepunkte an den Stellen $\mu\pm\sigma$. Zudem Gilt:
\begin{gather*}
	E(X)=\mu \\
	\sigma(X)=\sigma
\end{gather*}
Falls $\mu = 0$ und $\sigma = 1$ nennt man die normalverteilte Zufallsgröße $X$ auch Standartnormalfunktion $\Phi$. Für diese gilt:
\begin{gather*}
	\Phi(-z) = 1-\Phi(z) \text{ mit } z\in\mathbb{R}
\end{gather*} 
Falls $Y:=a\cdot X+b$ mit $a,b\in\mathbb{R}$, $a\neq 0$ und $X\sim N(\mu,\sigma^2)$ dann gilt:
\begin{gather*}
	Y\sim N(a\mu+b,a^2\sigma^2)\\
	\frac{X-\mu}{\sigma}\sim N(0,1)
\end{gather*}
Daher gilt weiter:
\begin{gather*}
	P(X\leq x)=\Phi\left(\frac{x-\mu}{\sigma}\right) \text{ mit } x\in\mathbb{R}\\
	P(X\in[a,b])=P(a\leq X\leq b)=\Phi\left(\frac{b-\mu}{\sigma}\right)-\Phi\left(\frac{a-\mu}{\sigma}\right) \text{ mit } a,b\in\mathbb{R} \text{ und } a<b
\end{gather*}
Für eine Normalverteilung $X\sim N(\mu,\sigma^2)$ mit $\alpha\in\mathbb{R}^+$ gilt:
\begin{equation*}
	P(\mu-\alpha<X<\mu+\alpha)=2\Phi\left(\frac{\alpha}{\sigma}\right)-1
\end{equation*}
Wenn nun $p\in[0,1]$ liegt und $\bar{x}$ mit $\Phi(\bar{x})=\frac{p+1}{2}$ ist, so gilt mit $\alpha=\sigma\cdot\bar{x}$:
\begin{equation*}
	P(\mu-\alpha<X\mu+\alpha)=p
\end{equation*}
Daher gilt, dass
\begin{itemize}
	\item $P(\mu-\sigma\leq X\leq\mu+\sigma)\approx\frac{2}{3}$
	\item $P(\mu-2\sigma\leq X\leq\mu+2\sigma)\approx 0.95$
	\item $P(\mu-3\sigma\leq X\leq\mu+3\sigma)\approx 0.9975$
\end{itemize}
\subsection{Zentraler Grenzwertsatz}
Für $X_1,X_2,X_3,...,X_n$ unabhängig und identisch verteile Zufallsvariablen mit $E(X_i)=\mu$ und $\sigma(X_i)=\sigma$ für $i=1,...,n$ und $\bar{X}:=\frac{1}{n}\sum_{i=1}^nX_i$ gilt:
\begin{gather*}
	Z_n:=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\rightarrow N(0,1)\\
	\lim_{n\rightarrow\infty}P(Z_n\leq x)=\Phi(x)
\end{gather*}
Für Daten unabhängiger, identischer Zufallsexperimente ist deren Mittelwert annäherned normalverteilt. Deren Erwartungswert und Standartabweichung lässt sich einfach empirisch bestimmen. 
\subsection{Hypothesentests}
\begin{itemize}
	\item \textbf{Nullhypothese} $H_0$: Annahme über ein erwartetes $\mu$
	\item \textbf{Alternativhypothese} $H_1$: Gegenereignis zu $H_0$ 
	\item Möglichkeiten der Hypothesen (Test eines Parameters $\theta$):\\
			\begin{tabular}{|l|l|l|}
				\hline
				Nullhypothese $H_0$ & Alternativhypothese $H_1$ & Art\\\hline
				$\theta = \theta_0$ & $\theta \neq \theta_0$ & zweiseitig\\\hline
				$\theta \leq \theta_0$ & $\theta > \theta_0$ & einseitig\\\hline
				$\theta \geq \theta_0$ & $\theta < \theta_0$ & einseitig\\\hline
			\end{tabular}
	\item Fehlerarten:\\
			\begin{tabular}{|l|c|c|}
				\hline
				\diagbox{Entscheidung}{Ground Truth}&$H_0$ ist wahr & $H_1$ ist wahr\\\hline
				Annahme von $H_0$ & richtig entschieden & Fehler zweiter Art \\\hline
				Ablehnung von $H_0$  & Fehler erster Art & richtig entschieden\\\hline
 			\end{tabular}
 	\item Annahme oder ablehnung von $H_0$:
 	\begin{itemize}
 		\item \textbf{Ablehung von} $H_0$: Abweichung zwischen Prüfgröße und Annahme ist signifikant. Somit wird die Nullhypothese $H_0$ verworfen, bzw. die Alternativhypothese $H_1$ - mit einer Irrtumswahrscheilichkeit von $\alpha$ angenommen.
 		\item \textbf{Annahme von } $H_0$: Es spricht nichts gegen eine Ablehnung/Verwerfung von $H_0$ damit wird diese Angenommen. 
 	\end{itemize}
\end{itemize}
Vorgehen zum aufstellen eines Hypothesentests:
\begin{enumerate}
	\item Aufstellen der Nullhypothese $H_0$ und Alternativhypothese $H_1$
	\item Festlegen des Signifikanzniveaus $\alpha$ (Wahrscheinlichkeit für einen Fehler erster Art)
	\item Berechnung des Streubereichs zum Signifikanzniveau $\alpha$
	\item Erhebung einer Stichbrube und berechnen der zugehörigen Prüfgröße
	\item Entscheidung über die Ablehnung oder Annahme von $H_0$
\end{enumerate}
\subsubsection{Hypothesentests bei Normalverteilungen}
Streubereiche für Normalverteilungen:\\
\begin{tabular}{|c|c|c|}
	\hline
	\textbf{$H_0$} & \textbf{$H_1$} & \textbf{Streubreich}\\\hline
	$\mu=\mu_0$ & $\mu\neq\mu_0$ & $\left[\mu_0-z_{1-\alpha/2\frac{\sigma}{\sqrt{n}}}, \mu_0+z_{1-\alpha/2\frac{\sigma}{\sqrt{n}}}\right]$\\\hline
	$\mu\geq\mu_0$ & $\mu<\mu_0$ & $\left[\mu_0-z_{1-\alpha/2\frac{\sigma}{\sqrt{n}}}, \infty\right)$\\\hline
	$\mu\leq\mu_0$ & $\mu>\mu_0$ & $\left(-\infty, \mu_0+z_{1-\alpha/2\frac{\sigma}{\sqrt{n}}}\right]$\\\hline
\end{tabular}\\
Für eine Stichprobe $x_1,...,x_n$ von $X$ mit $\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$ gilt:
\begin{gather*}
	\bar{x}\in \text{ Streubereich }\Rightarrow H_0 \text{ wird akzeptiert}\\
	\bar{x}\notin \text{ Streubereich }\Rightarrow H_0 \text{ wird abgelehnt}
\end{gather*}
\section{Fourierreihen}
\subsection{Gleichanteil / Mittelwert}
\begin{equation*}
	m=\frac{\text{Integral über eine Periode}}{\text{Periode}}=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)dt
\end{equation*}
\subsection{Trigonometrisches Polynom}
Darstellung einer Funktion durch ein Vielfaches aus Sinus und Kosinus mit unterschiedlichen vielfachen an Kreisfrequenzen.
\begin{equation*}
	p_n(t)=\frac{a_0}{2}+\sum\limits_{k=1}^n(a_kcos(k\omega t)+b_ksin(k\omega t)), \quad \omega=\frac{2\pi}{T}
\end{equation*}
Man nennt diese Funktion \textbf{trigonometrisches Polynom} vom Grad $n$. $a_0,a_1,...,a_n$ und $b_1,...,b_n$ sind beliebige Zahlen, mit $a_n, b_n \neq 0$.\\
Für jede Funktion $f(t)$ mit Periode $T>0$ lässt sich als Fourierreihe darstellen.\\
Die Fourierreihe konvergiert
\begin{enumerate}
	\item gleichmäßig (gleicher Fehler an allen Stellen) gegen $f$ wenn $f$ stetig (ohne Sprungstellen) und abschnittsweise stetig differenzierbar ist,
	\item punktweise gegen $\frac{1}{2}(f(t+0)+f(t-0)$, das arithmetische Mittel aus links- und rechtsseiteigem Grenzwert, wenn $f$ aus endlich vielen stetigen Abschnitten besteht (mit Sprungstellen),
	\item und somit konvergiert sie in den abgeschlossenen Intervallen in denen $f$ stetig ist - dort sogar gleichmäßig.
\end{enumerate}
\subsection{Berechnung der Fourierkoeffizienten}
\begin{gather*}
	a_k=\frac{2}{T}\int\limits_{-\frac{T}{2}}^{\frac{T}{2}}f(t)cos(k\omega t)dt, \quad k=1,2,...\\
	b_k=\frac{2}{T}\int\limits_{-\frac{T}{2}}^{\frac{T}{2}}f(t)sin(k\omega t)dt, \quad k=1,2,...\\
\end{gather*}
Für periodische \textbf{stetige Funktionen Konvergiert} die Fourierreihe schnell, da ihre Koeffizienten $a_k,b_k$ zu $1/k^2$ proportional abklingen. Für Funktionen mit Sprungstellen klingen sie nur zu $1/k$ proportional ab.
\subsection{Gibbs'sches Phänomen}
Bei z.B. nachbildung einer Rechteckfunktion mittels der Fourierreihe bleibt ein Überschwinger kurz nach der Flanke von $\sim 9\%$ bestehen (und auch nicht entfernen).
\subsection{Komplexe Fourierreihe}
Darstellung einer Funktion $f$ mit Periode $T$ als unendliche Summe heißt \textbf{komplexe Fourierreihe}
\begin{equation*}
	f(t) = \sum\limits_{k=-\infty}^{\infty}c_ke^{ik\omega t}, \quad \omega=\frac{2\pi}{T}
\end{equation*}
Der zusammenhang zwischen der \textbf{komplexen Fourrierkoeffizienten} zu den reellen Fourierkoeffizienten besteht aus:
\begin{gather*}
	a_0=2\Re(c_0), \quad c_0=\frac{a_0}{2}\\
	a_k=2\Re(c_k), \quad c_k=\frac{a_k-ib_k}{2}, \quad k=1,2,3,...\\
	b_k=-2\Im(c_k), \quad c_k=\frac{a_k+ib_k}{2}, \quad k=-1,-2,-3,...
\end{gather*}
\subsection{Berechnung komplexer Fourierkoeffizienten}
\begin{equation*}
	c_k=\frac{1}{T}\int\limits_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-ik\omega t}dt, \quad k=0,\pm1,\pm2,...
\end{equation*}
\subsection{Ähnlichkeit, Zeitumkehr und Zeitverschiebung}
\begin{itemize}
	\item \textbf{Ähnlichkeit}:\\
			Die Funktionen $f(t)$ und $\tilde{f}(t)=f(at)$ haben dieselben Fourierkoeffizienten. Die beiden Fourierreihen haben lediglich unterschiedliche Perioden und Kreisfrequenzen.
	\item \textbf{Zeitumkehr}:\\
			Die Funktionen $f(t)$ und $\tilde{f}(t)=f(-t)$ haben dieselbe Periode $T$ und Kreisfrequenz $\omega$. Zwischen den Fourierkoeffizienten, Spektrum und Phase besteht der Zusammenhang:
			\begin{equation*}
				\tilde{a_k}=a_k,\tilde{b_k}=-b_k,\tilde{c_k}=\overline{c_k},\tilde{A_k}=A_k,\tilde{\varphi_k}=\varphi_k, 
			\end{equation*}
	\item\textbf{Zeitumkehr}:\\
			Die Funktionen $f(t)$ und $\tilde{f}(t)=f(t-t_0)$ haben dieselben Anplituden $\tilde{A_k}=A_k$ und Phasenwinkel $\tilde{\varphi_k}=-k\omega t_0 + \varphi_k$
\end{itemize}
\section{Verallgemeinerte Funktionen}
\subsection{Heavisiede-Funktion}
\begin{equation*}
	\sigma(t)=\begin{cases}
		0 \text{ für } t<0\\
		1 \text{ für } t\geq 1
	\end{cases}
\end{equation*}
\subsection{Rechteckpuls}
\begin{equation*}
	r(t)=\sigma(t-t_0)-\sigma(t-t_1)
\end{equation*}
Der Rechteckpuls lässt die Funktion $f$ außerhalb des Intervalls $[t_0,t_1]$ ausblenden, was zu einer neuen Funktion $g(t)$ führt:
\begin{equation*}
	g(t):=f(t)r(t)=\begin{cases}
		0 \text{ für } t<t_0\\
		f(t) \text{ für } t_0\leq t \leq t_1\\
		0 \text{ für } t>t_1
	\end{cases}
\end{equation*}
\subsection{Dirac-Puls}
Die Rechteckfunktion $d_\varepsilon$ mit konstantem Flächeninhalt 1:
\begin{equation*}
	d_\varepsilon=\frac{1}{\varepsilon}(\sigma(t)-\sigma(t-\varepsilon))
\end{equation*}
Wird im Grenzwert zum \textbf{Dirac-Puls}/\textbf{Dirac-Distribution}:
\begin{equation*}
	\delta(t)=\lim_{\varepsilon\rightarrow 0}d_\varepsilon(t)
\end{equation*}
Wenn man eine Funktion $f$ mit dem Dirac-Puls $\delta(t-t0)$ multipliziert, so werden alle Funktionswerte außerhalb $t_0$ ausgeblendet.
\begin{gather*}
	\int_{-\infty}^{\infty}f(t)\delta(t-t_0)dt=f(t_0)\\
	\int_{-\infty}^{\infty}\delta(t-t_0)dt=1
\end{gather*}
Die \textbf{varallgemeintere Ableitung} der Heaviside-Funktion ist der Dirac-Puls:
\begin{gather*}
	\dot{\sigma}(t)=\delta(t)\\
	\int\delta(t)dt=\sigma(t)+C
\end{gather*}
\subsection{Faltung}
\begin{equation*}
	h(t) = f(t)\star g(t)=\int_{-\infty}^\infty f(\tau)g(t-\tau)d\tau
\end{equation*}
Rechenregeln (mit Funktionen $f,g,h$ und Konstante $C$):
\begin{itemize}
	\item $f\star g = g\star f$
	\item $C(f\star g) = (Cf)\star g=f\star (Cg)$
	\item $f\star (g\star h)=(f\star g)\star h$
	\item $f\star (g + h)=(f\star g) + (f\star h)$
\end{itemize}
Eine Faltung einer Funktion $f$ mit dem Dirac-Puls $\delta$ lässt die Funktion $f$ unverändert.\\
\subsection{Einseitige Faltung}
Sind beide Funktionen $f$ und $g$ für negative Argumente null, dann berechnet sich die einseitige Faltung durch:
\begin{equation*}
	f(t)\star g(t)=\int_0^tf(\tau)g(t-\tau)d\tau
\end{equation*}
\section{Fouriertransformation}
Durch die Fouriertransformation wird einer Funktion $s$ im Zeitbereich eine Funktion $S$ im Frequenzbereich zugeordnet.
\begin{equation*}
	s(t)\laplace S(f)=\int_{-\infty}^\infty s(t)e^{-2\pi ift}dt
\end{equation*}
Eigenschaften:
\begin{itemize}
	\item \textbf{Linearität}:\\
			Eine Addition von Funktionen im Zeitbereich entspricht einer Addition der Fouriertransformierten im Frequenzbereich.\\
			Eine Multiplikation einer Funktion mit einem konstanten Faktor im Zeitbereich entspricht der Multiplikation mit demselben Faktor im Frequenzbereich.
			\begin{eqnarray*}
				s_1(t),s_2(t) & \laplace & S_1(f),S_2(f)\\
				\downarrow & & \downarrow \\
				C_1s_1(t)+C_2s_2(t) & \laplace & C_1S_1(f)+C_2S_2(f)
			\end{eqnarray*}
	\item \textbf{Zeitverschiebung}:\\
			Eine Verschiebung der Funktion $s$ um $t_0$ im Zeitbereich entspricht der Multiplikation mit $e^{-i2\pi ft_0}$ im Frequenzbereich.
			\begin{eqnarray*}
				s(t) & \laplace & S(f)\\
				\downarrow & & \downarrow \\
				s(t-t_0) & \laplace & e^{-i2\pi ft_0}S(f)
			\end{eqnarray*}
	\item \textbf{Frequenzverschiebung}:\\
			Eine Verschiebung der Funktion $S$ um $f_0$ im Frequenzbereich entspricht der Multiplikation mit $e^{-2\pi f_0 t}$ im Zeitbereich.
			\begin{eqnarray*}
				S(f) & \Laplace & s(t)\\
				\downarrow & & \downarrow \\
				S(f-f_0) & \Laplace & e^{-i2\pi f_0t}s(t)
			\end{eqnarray*}
\end{itemize}
\subsection{Fouriertransformation einer nicht abklingenden Funktion}
\todo{Seite 122 Beispiel 7.6}
\section{Laplacetransformation}
Durch die Laplacetransformation wird einer Funktion $f$ im Zeitbereich eine Funktion $F$ im komplexen Bildbereich zugeordnet.
\begin{equation*}
	f(t)\laplace F(s)=\int_0^\infty f(t)e^{-st}dt, \quad t\in\mathbb{R}, s\in\mathbb{C}
\end{equation*}
Die Fouriertransformierte einer Funktion geht aus der Laplacetransformierten durch ersetzen von $s=2\pi if$ hervor.\\
Eigenschaften:
\begin{itemize}
	\item \textbf{Linearität}:\\
			Eine Addition von Funktionen im Zeitbereich entspricht einer Addition der Laplacetransformierten im Bildbereich.\\
			Eine Multiplikation einer Funktion mit einem konstanten Faktor im Zeitbereich entspricht der Multiplikation mit demselben Faktor im Bildbereich.
			\begin{eqnarray*}
				f_1(t),f_2(t) & \laplace & F_1(s),F_2(s)\\
				\downarrow & & \downarrow \\
				C_1f_1(t)+C_2f_2(t) & \laplace & C_1F_1(s)+C_2F_2(s)
			\end{eqnarray*}
	\item \textbf{Ähnlichkeit}:\\
			Eine Ersetzung von $t$ durch $at$ der Funktion $f$ im Zeitbereich entspricht der Ersetzung von $s$ durch $\frac{s}{a}$im Bildbereich und einer Division der Laplacetransformierten durch $a$.
			\begin{eqnarray*}
				f(t) & \laplace & F(s)\\
				\downarrow & & \downarrow \\
				f(at) & \laplace & \frac{1}{a} F\left(\frac{s}{a}\right)
			\end{eqnarray*}
	\item \textbf{Verschiebung im Bildbereich/Dämpfungssatz}:\\
			Eine Verschiebung der Funktion $F$ um $s_0$ im Bildbereich entspricht der Multiplikation mit $e^{-s_0 t}$ im Zeitbereich.
			\begin{eqnarray*}
				F(s) & \Laplace & f(t)\\
				\downarrow & & \downarrow \\
				F(s-s_0) & \Laplace & e^{-s_0t}f(t)
			\end{eqnarray*}
\end{itemize}
\subsection{Differenziation und Integration}
\begin{itemize}
	\item \textbf{Differenziation im Zeitbereich}:\\
			\begin{eqnarray*}
				f(t) & \laplace & F(s)\\
				\downarrow & & \downarrow \\
				f'(t) & \laplace & s F\left(s\right) -f(0)
			\end{eqnarray*}
	\item \textbf{Höhere Ableitungen im Zeitbereich}:\\
			\begin{eqnarray*}
				f'(t) & \laplace & s F\left(s\right) -f(0)\\
				f''(t) & \laplace & s^2 F\left(s\right) -sf(0)-f'(0)\\
				... & & \\
				f^{(n)}(t) & \laplace & s^nF(s)-s^{n-1}f(0)- \dotsm -sf^{n-2}(0) - f^{n-1}(0)
			\end{eqnarray*}
	\item \textbf{Integration im Zeitbereich}:\\
		 	\begin{eqnarray*}
				f(t) & \laplace & F(s)\\
				\downarrow & & \downarrow \\
				\int_0^tf(\tau)d\tau & \laplace & \frac{1}{s} F\left(s\right)
			\end{eqnarray*}
	\item \textbf{Integration im Bildbereich}:\\
		 	\begin{eqnarray*}
				F(s) & \Laplace & f(t)\\
				\downarrow & & \downarrow \\
				\int_s^\infty F(u)du & \Laplace & \frac{1}{t} f\left(s\right)
			\end{eqnarray*}
	\item \textbf{Faltung im Zeitbereich}:\\
			\begin{eqnarray*}
				f_1(t),f_2(t) & \laplace & F_1(s),F_2(s)\\
				\downarrow & & \downarrow \\
				f_1(t)\star f_2(t) & \laplace & F_1(s)\cdot F_2(s)
			\end{eqnarray*}
\end{itemize}
\subsection{Rücktransformation}
Die Rücktrnsformation geschieht durch eine Korrespondenztabelle.
\subsection{Lösung von gewöhnlichen DGL mit Laplacetransformation}
\begin{itemize}
	\item Transformation der Differenzialgleichung in den Bildbereich.
	\item Lösung der algebraischen Gleichung im Bildbreich.
	\item Rücktransformation mittels Korrespondenztabelle.
\end{itemize}
\begin{eqnarray*}
	\text{Differentialgleichung} & \laplace & \text{Algebraische Gleichung}\\
	\downarrow & & \downarrow \\
	\text{Lösung Zeitbereich} & \laplace & \text{Lösung Bildbereich}
\end{eqnarray*}
\subsection{LTI-Systeme}
\todo{Zusammenfassung LTI systeme notwendig?}
\section{Zusatz}
\subsection{Integration}
\begin{equation*}
	\int\limits_{a}^{b}f(x) dx = [F(x)]_a^b = F(b) - F(a)
\end{equation*}
\begin{equation*}
	\int\limits_{a}^{b}f(x) + g(x) dx = \int\limits_{a}^{b}f(x) dx + \int\limits_{a}^{b}g(x) dx
\end{equation*}
\begin{equation*}
	\int\limits_{a}^{b}c\cdot f(x) dx = c\cdot \int\limits_{a}^{b}f(x) dx
\end{equation*}
\subsection{Partielle Integration}
\begin{equation*}
	u(x)\cdot v(x) = \int u'(x) \cdot v(x) dx + \int u(x) \cdot v'(x) dx
\end{equation*}
\todo{Integrations regeln}
\subsection{Partialbruchzerlegung}
\todo{Partialbruchzerlegung basics}
\subsection{Differentialgleichungen}
\todo{Basics DGL Lösungen}
\newpage
\subsection{Standartnormalverteilungstabelle}
\begin{table}[H]
	\begin{adjustbox}{width=\columnwidth,center}
	\begin{tabular}{|>{\bfseries}x|l|l|l|l|l|l|l|l|l|l|}
	\hline
	\rowcolor{LightCyan}
		z & \textbf{0} & \textbf{0,01} & \textbf{0,02} & \textbf{0,03} & \textbf{0,04} & \textbf{0,05} & \textbf{0,06} & \textbf{0,07} & \textbf{0,08} & \textbf{0,09}\\\hline
	0,0 & 0,50000 & 0,50399 & 0,50798 & 0,51197 & 0,51595 & 0,51994 & 0,52392 & 0,52790 & 0,53188 & 0,53586\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,1 & 0,53983 & 0,54380 & 0,54776 & 0,55172 & 0,55567 & 0,55962 & 0,56356 & 0,56749 & 0,57142 & 0,57535\\\hline
	0,2 & 0,57926 & 0,58317 & 0,58706 & 0,59095 & 0,59483 & 0,59871 & 0,60257 & 0,60642 & 0,61026 & 0,61409\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,3 & 0,61791 & 0,62172 & 0,62552 & 0,62930 & 0,63307 & 0,63683 & 0,64058 & 0,64431 & 0,64803 & 0,65173\\\hline
	0,4 & 0,65542 & 0,65910 & 0,66276 & 0,66640 & 0,67003 & 0,67364 & 0,67724 & 0,68082 & 0,68439 & 0,68793\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,5 & 0,69146 & 0,69497 & 0,69847 & 0,70194 & 0,70540 & 0,70884 & 0,71226 & 0,71566 & 0,71904 & 0,72240\\\hline
	0,6 & 0,72575 & 0,72907 & 0,73237 & 0,73565 & 0,73891 & 0,74215 & 0,74537 & 0,74857 & 0,75175 & 0,75490\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,7 & 0,75804 & 0,76115 & 0,76424 & 0,76730 & 0,77035 & 0,77337 & 0,77637 & 0,77935 & 0,78230 & 0,78524\\\hline
	0,8 & 0,78814 & 0,79103 & 0,79389 & 0,79673 & 0,79955 & 0,80234 & 0,80511 & 0,80785 & 0,81057 & 0,81327\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}0,9 & 0,81594 & 0,81859 & 0,82121 & 0,82381 & 0,82639 & 0,82894 & 0,83147 & 0,83398 & 0,83646 & 0,83891\\\hline
	1,0 & 0,84134 & 0,84375 & 0,84614 & 0,84849 & 0,85083 & 0,85314 & 0,85543 & 0,85769 & 0,85993 & 0,86214\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,1 & 0,86433 & 0,86650 & 0,86864 & 0,87076 & 0,87286 & 0,87493 & 0,87698 & 0,87900 & 0,88100 & 0,88298\\\hline
	1,2 & 0,88493 & 0,88686 & 0,88877 & 0,89065 & 0,89251 & 0,89435 & 0,89617 & 0,89796 & 0,89973 & 0,90147\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,3 & 0,90320 & 0,90490 & 0,90658 & 0,90824 & 0,90988 & 0,91149 & 0,91309 & 0,91466 & 0,91621 & 0,91774\\\hline
	1,4 & 0,91924 & 0,92073 & 0,92220 & 0,92364 & 0,92507 & 0,92647 & 0,92785 & 0,92922 & 0,93056 & 0,93189\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,5 & 0,93319 & 0,93448 & 0,93574 & 0,93699 & 0,93822 & 0,93943 & 0,94062 & 0,94179 & 0,94295 & 0,94408\\\hline
	1,6 & 0,94520 & 0,94630 & 0,94738 & 0,94845 & 0,94950 & 0,95053 & 0,95154 & 0,95254 & 0,95352 & 0,95449\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,7 & 0,95543 & 0,95637 & 0,95728 & 0,95818 & 0,95907 & 0,95994 & 0,96080 & 0,96164 & 0,96246 & 0,96327\\\hline
	1,8 & 0,96407 & 0,96485 & 0,96562 & 0,96638 & 0,96712 & 0,96784 & 0,96856 & 0,96926 & 0,96995 & 0,97062\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}1,9 & 0,97128 & 0,97193 & 0,97257 & 0,97320 & 0,97381 & 0,97441 & 0,97500 & 0,97558 & 0,97615 & 0,97670\\\hline
	2,0 & 0,97725 & 0,97778 & 0,97831 & 0,97882 & 0,97932 & 0,97982 & 0,98030 & 0,98077 & 0,98124 & 0,98169\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,1 & 0,98214 & 0,98257 & 0,98300 & 0,98341 & 0,98382 & 0,98422 & 0,98461 & 0,98500 & 0,98537 & 0,98574\\\hline
	2,2 & 0,98610 & 0,98645 & 0,98679 & 0,98713 & 0,98745 & 0,98778 & 0,98809 & 0,98840 & 0,98870 & 0,98899\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,3 & 0,98928 & 0,98956 & 0,98983 & 0,99010 & 0,99036 & 0,99061 & 0,99086 & 0,99111 & 0,99134 & 0,99158\\\hline
	2,4 & 0,99180 & 0,99202 & 0,99224 & 0,99245 & 0,99266 & 0,99286 & 0,99305 & 0,99324 & 0,99343 & 0,99361\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,5 & 0,99379 & 0,99396 & 0,99413 & 0,99430 & 0,99446 & 0,99461 & 0,99477 & 0,99492 & 0,99506 & 0,99520\\\hline
	2,6 & 0,99534 & 0,99547 & 0,99560 & 0,99573 & 0,99585 & 0,99598 & 0,99609 & 0,99621 & 0,99632 & 0,99643\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,7 & 0,99653 & 0,99664 & 0,99674 & 0,99683 & 0,99693 & 0,99702 & 0,99711 & 0,99720 & 0,99728 & 0,99736\\\hline
	2,8 & 0,99744 & 0,99752 & 0,99760 & 0,99767 & 0,99774 & 0,99781 & 0,99788 & 0,99795 & 0,99801 & 0,99807\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}2,9 & 0,99813 & 0,99819 & 0,99825 & 0,99831 & 0,99836 & 0,99841 & 0,99846 & 0,99851 & 0,99856 & 0,99861\\\hline
	3,0 & 0,99865 & 0,99869 & 0,99874 & 0,99878 & 0,99882 & 0,99886 & 0,99889 & 0,99893 & 0,99896 & 0,99900\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,1 & 0,99903 & 0,99906 & 0,99910 & 0,99913 & 0,99916 & 0,99918 & 0,99921 & 0,99924 & 0,99926 & 0,99929\\\hline
	3,2 & 0,99931 & 0,99934 & 0,99936 & 0,99938 & 0,99940 & 0,99942 & 0,99944 & 0,99946 & 0,99948 & 0,99950\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,3 & 0,99952 & 0,99953 & 0,99955 & 0,99957 & 0,99958 & 0,99960 & 0,99961 & 0,99962 & 0,99964 & 0,99965\\\hline
	3,4 & 0,99966 & 0,99968 & 0,99969 & 0,99970 & 0,99971 & 0,99972 & 0,99973 & 0,99974 & 0,99975 & 0,99976\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,5 & 0,99977 & 0,99978 & 0,99978 & 0,99979 & 0,99980 & 0,99981 & 0,99981 & 0,99982 & 0,99983 & 0,99983\\\hline
	3,6 & 0,99984 & 0,99985 & 0,99985 & 0,99986 & 0,99986 & 0,99987 & 0,99987 & 0,99988 & 0,99988 & 0,99989\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,7 & 0,99989 & 0,99990 & 0,99990 & 0,99990 & 0,99991 & 0,99991 & 0,99992 & 0,99992 & 0,99992 & 0,99992\\\hline
	3,8 & 0,99993 & 0,99993 & 0,99993 & 0,99994 & 0,99994 & 0,99994 & 0,99994 & 0,99995 & 0,99995 & 0,99995\\\hline\rowcolor{Gray}
	\cellcolor{LightCyan}3,9 & 0,99995 & 0,99995 & 0,99996 & 0,99996 & 0,99996 & 0,99996 & 0,99996 & 0,99996 & 0,99997 & 0,99997\\\hline
	4,0 & 0,99997 & 0,99997 & 0,99997 & 0,99997 & 0,99997 & 0,99997 & 0,99998 & 0,99998 & 0,99998 & 0,99998\\\hline
	\end{tabular}
	\end{adjustbox}
\end{table}
\newpage
\subsection{Korrespondenztabelle}
\begin{tabular}{r|c|c||r|c|c}
	Nr. & Bildfunktion $F(s)$ & Zeitfunktion $f(t)$ & Nr. & Bildfunktion $F(s)$ & Zeitfunktion $f(t)$ \\\hline
	1 & $1$ & $\delta(t)$ & 17 & $\frac{a}{s^2+a^2}$ & $\sin at$\\
	2 & $\frac{1}{s}$ & $1$ & 18 & $\frac{s}{s^2+a^2}$ & $\cos at$\\
	3 & $\frac{1}{s^2}$ & $t$ & 19 & $\frac{a}{s^2-a^2}$ & $\sinh at$\\
	4 & $\frac{n!}{s^n}$ & $t^n$ & 20 & $\frac{s}{s^2-a^2}$ & $\cosh at$\\\hline
	5 & $\frac{1}{s-a}$ & $e^{at}$ & 21 & $\frac{a}{(s-b)^2+a^2}$ & $e^{bt}\sin at$\\
	6 & $\frac{1}{(s-a)^2}$ & $te^{at}$ & 22 & $\frac{s-b}{(s-b)^2+a^2}$ & $e^{bt}\cos at$\\
	7 & $\frac{a}{s(s-a)}$ & $e^{at}-1$ & 23 & $\frac{a}{(s-b)^2-a^2}$ & $e^{bt}\sinh at$\\
	8 & $\frac{a-b}{(s-a)(s-b)}$ & $e^{at}-e^{bt}$ & 24 & $\frac{s-b}{(s-b)^2-a^2}$ & $e^{bt}\cosh at$\\\hline
	9 & $\frac{a}{1+as}$ & $e^{-\frac{t}{a}}$ & 25 & $\frac{2as}{(s^2+a^2)^2}$ & $t\sin at$\\
	10 & $\frac{a^2}{(1+as)^2}$ & $te^{-\frac{t}{a}}$ & 26 & $\frac{s^2-a^2}{(s^2+a^2)^2}$ & $t\cos at$\\
	11 & $\frac{1}{s(1+as)}$ & $1-e^{-\frac{t}{a}}$ & 27 & $\frac{2as}{(s^2-a^2)^2}$ & $t\sinh at$\\
	12 & $\frac{a-b}{(1+as)(1+bs)}$ & $e^{-\frac{t}{a}}-e^{-\frac{t}{b}}$ & 28 & $\frac{s^2+a^2}{(s^2-a^2)^2}$ & $t\cosh at$\\\hline
	13 & $\frac{s}{(s-a)^2}$ & $(1+at)e^{at}$ & 29 & $\frac{2}{(s-a)^3}$ & $t^2e^{at}$\\
	14 & $\frac{(a-b)s}{(s-a)(s-b)}$ & $ae^{at}+be^{bt}$ & 30 & $\frac{2s}{(s-a)^3}$ & $(at^2+2t)e^{at}$\\
	15 & $\frac{a^3s}{(1+as)^2}$ & $(a-t)e^{-\frac{t}{a}}$ & 31 & $\frac{2s^2}{(s-a)^3}$ & $(a^2t^2+4at+2)e^{at}$\\
	16 & $\frac{ab(a-b)s}{(1+as)(1+bs)}$ & $ae^{-\frac{t}{b}}-be^{-\frac{t}{a}}$ & 32 & $\frac{a^2}{s^2(s-a)}$ & $e^{at}-at-1$\\\hline
\end{tabular}
\newpage
\pagenumbering{Alph}
\section{Anhang}
\printbibliography[heading=subbibnumbered]
\end{document}